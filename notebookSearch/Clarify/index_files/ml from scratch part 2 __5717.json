{"name": "ml from scratch part 2 ", "full_name": " h2 Hello Kagglers h3 Checking The Data h3 Basic EDA h3 Brief Analysis of the data h3 Analysis Of Diabetic Cases h3 PairPlots h3 Observations h3 Let s Do Some Predictive Modeling h3 Stratification h3 SVM h3 Logistic Regression h3 Decision Tree h3 K Nearest Neighbours h3 In a Nutshell h2 Feature Extraction Selection h3 Correlation Matrix h3 Observations h3 Random Forest Classifier h3 Observations h2 Standardisation h3 Using Only Important Features And Standardisation h2 Cross Validation h2 Ensembling h3 Linear and Radial SVM h3 Linear SVM with Logistic Regression h3 Logistic Regression with Radial SVM h3 All 3 classifiers combined ", "stargazers_count": 0, "forks_count": 0, "description": " Hello Kagglers This notebook is sort of a guide to some important concepts in Machine Learning like 1 Feature Selection 2 Standardisation 3 Ensembling etc I have tried to explain the general idea behind these concepts so that it is helpful for begineers also If your are completely new to Machine Learning I would suggest you to go through this basic tutorial to ML https www kaggle com ash316 ml from scratch with iris notebook If You find this notebook useful PLEASE UPVOTE Checking The Data The data looks clean So we can start with the analysis Basic EDA Brief Analysis of the data Analysis Of Diabetic Cases PairPlots Lets us see the distribution of the features in the dataset Observations 1 The diagonal shows the distribution of the the dataset with the kernel density plots 2 The scatter plots shows the relation between each and every attribute or features taken pairwise Looking at the scatter plots we can say that no two attributes are able to clearly seperate the two outcome class instances Let s Do Some Predictive Modeling Stratification When we split the dataset into train and test datasets the split is completely random Thus the instances of each class label or outcome in the train or test datasets is random Thus we may have many instances of class 1 in training data and less instances of class 2 in the training data So during classification we may have accurate predictions for class1 but not for class2 Thus we stratify the data so that we have proportionate data for all the classes in both the training and testing data SVM Logistic Regression Decision Tree K Nearest Neighbours In a Nutshell The above algorithms are not giving vey high accuracy This can be improved by using Feature Selection and using only relevant features We can also use ensembling or averaging of different algorithms I will do these things in subsequent parts Feature Extraction Selection 1 A lot many features can affect the accuracy of the algorithm 2 Feature Extraction means to select only the important features in order to improve the accuracy of the algorithm 3 It reduces training time and reduces overfitting 4 We can choose important features in 2 ways a Correlation matrix selecting only the uncorrelated features b RandomForestClassifier It gives the importance of the features Correlation Matrix Observations 1 All the features look to be uncorrelated So we cannot eliminate any features just by looking at the correlation matrix Random Forest Classifier Observations 1 The important features are Glucose BMI Age DiabetesPedigreeFunction Standardisation There can be a lot of deviation in the given dataset An example in the dataset can be the BMI where it has 248 unique values This high variance has to be standardised Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1 Using Only Important Features And Standardisation The above dataframe shows the new accuracy of the models after feature selection We can see that the Accuarcy for linear Svm increases by 1 whereas it increases about 12 for Radial Svm For LR the accuracy decreases Cross Validation Many a times the data is imbalanced i e there may be a high number of class1 instances but less number of other class instances Thus we should train and test our algorithm on each and every instance of the dataset Then we can take an average of all the noted accuracies over the dataset 1 The K Fold Cross Validation works by first dividing the dataset into k subsets 2 Let s say we divide the dataset into k 5 parts We reserve 1 part for testing and train the algorithm over the 4 parts 3 We continue the process by changing the testing part in each iteration and training the algorithm over the other parts The accuracies and errors are then averaged to get a average accuracy of the algorithm This is called K Fold Cross Validation 4 An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set Thus with cross validation we can achieve a generalised model The above boxplot shows that SVM and LR perform the best while Decision Tree performs the worst Ensembling Ensemble methods are techniques that create multiple models and then combine them to produce improved results Ensemble methods usually produces more accurate solutions than a single model would The models used to create such ensemble models are called base models We will do ensembling with the Voting Ensemble Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms It works by first creating two or more standalone models from your training dataset A Voting Classifier can then be used to wrap your models and average the predictions of the sub models when asked to make predictions for new data We will be using weighted Voting Classifier We will assign to the classifiers according to their accuracies So the classifier with single accuracy will be assigned the highest weight and so on In our case we will use the Top 3 classifiers i e Linaer SVM Radial rbf SVM and Logistic Regression classifiers Linear and Radial SVM Linear SVM with Logistic Regression Logistic Regression with Radial SVM All 3 classifiers combined So the maximum Accuracy which we could get by using ensemble models is 78 125 I would like you all for having a look at this notebook Please Upvote if Useful  This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python docker image https github com kaggle docker python For example here s several helpful packages to load in Input data files are available in the input directory For example running this by clicking run or pressing Shift Enter will list the files in the input directory Any results you write to the current directory are saved as output stratify the outcome Gaussian Standardisation for K fold cross validation score evaluation k 10 split the data into 10 equal parts for Voting Classifier ", "id": "ash316/ml-from-scratch-part-2", "size": "5717", "language": "python", "html_url": "https://www.kaggle.com/code/ash316/ml-from-scratch-part-2", "git_url": "https://www.kaggle.com/code/ash316/ml-from-scratch-part-2"}