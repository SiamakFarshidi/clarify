{"name": "hepatitis using svm ", "full_name": " h2 Dataset Reading and Pre Processing steps h4 4 Check the datatype of each variable h4 5 Drop columns which are not significant h4 6 Identify the Categorical Columns and store them in a variable cat cols and numerical into num cols h4 7 Checking the null values h4 8 Split the data into X and y h4 9 Split the data into X train X test y train y test with test size 0 20 using sklearn h4 10 Check null values in train and test check value counts in y train and y test h4 11 Impute the Categorical Columns with mode and Numerical columns with mean h4 Convert all the categorical columns to Integer Format before dummification 2 0 as 2 etc h4 12 Dummify the Categorical columns h4 13 Scale the numeric attributes age bili alk sgot albu protime h2 MODEL BUILDING SVM h4 Non Linear SVM RBF h3 SVM with Grid Search for Paramater Tuning ", "stargazers_count": 0, "forks_count": 0, "description": " Attribute information 1 target DIE 1 LIVE 2 2 age 10 20 30 40 50 60 70 80 3 gender male 1 female 2 no 2 yes 1 4 steroid no yes 5 antivirals no yes 6 fatique no yes 7 malaise no yes 8 anorexia no yes 9 liverBig no yes 10 liverFirm no yes 11 spleen no yes 12 spiders no yes 13 ascites no yes 14 varices no yes 15 histology no yes 16 bilirubin 0 39 0 80 1 20 2 00 3 00 4 00 17 alk 33 80 120 160 200 250 18 sgot 13 100 200 300 400 500 19 albu 2 1 3 0 3 8 4 5 5 0 6 0 20 protime 10 20 30 40 50 60 70 80 90 NA s are represented with Dataset Reading and Pre Processing steps import required libraries 1 Read the HEPATITIS dataset and check the data shapes 2 Check basic summary statistics of the data 3 Check for value counts in target variable 4 Check the datatype of each variable 5 Drop columns which are not significant 6 Identify the Categorical Columns and store them in a variable cat cols and numerical into num cols 7 Checking the null values 8 Split the data into X and y 9 Split the data into X train X test y train y test with test size 0 20 using sklearn 10 Check null values in train and test check value counts in y train and y test 11 Impute the Categorical Columns with mode and Numerical columns with mean Convert all the categorical columns to Integer Format before dummification 2 0 as 2 etc 12 Dummify the Categorical columns 13 Scale the numeric attributes age bili alk sgot albu protime MODEL BUILDING SVM Non Linear SVM RBF Radial Basis Function is a commonly used kernel in SVC br img src rbf kernel png where math xmlns http www w3 org 1998 Math MathML mrow class MJX TeXAtom ORD mo stretchy false mo mrow mrow class MJX TeXAtom ORD mo stretchy false mo mrow mrow class MJX TeXAtom ORD mi mathvariant bold x mi sub i sub mrow mo x2212 mo mrow class MJX TeXAtom ORD msup mi mathvariant bold x mi sub j sub msup mrow mrow class MJX TeXAtom ORD mo stretchy false mo mrow msup mrow class MJX TeXAtom ORD mo stretchy false mo mrow mrow class MJX TeXAtom ORD sup 2 sup mrow msup math is the squared Euclidean distance between two data points x sub i sub and x sub j sub It is only important to know that an SVC classifier using an RBF kernel has two parameters gamma and C strong Gamma strong Gamma is a parameter of the RBF kernel and can be thought of as the spread of the kernel and therefore the decision region When gamma is low the curve of the decision boundary is very low and thus the decision region is very broad When gamma is high the curve of the decision boundary is high which creates islands of decision boundaries around data points strong C strong C is a parameter of the SVC learner and is the penalty for misclassifying a data point When C is small the classifier is okay with misclassified data points high bias low variance When C is large the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points low bias high variance strong Kernel Trick strong br Image you have a two dimensional non linearly separable dataset you would like to classify it using SVM It looks like not possible because the data is not linearly separable However if we transform the two dimensional data to a higher dimension say three dimension or even ten dimension we would be able to find a hyperplane to separate the data img src kernel trick png The problem is if we have a large dataset containing say millions of examples the transformation will take a long time to run br To solve this problem we actually only care about the result of the dot product x sub i sub x sub j sub br br If there is a function which could calculate the dot product and the result is the same as when we transform the data into higher dimension it would be fantastic This function is called a kernel function br br In essence what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions SVM with Grid Search for Paramater Tuning  Code to ignore warnings target 1 Die 2 Live null values in train null values in test Impute on train df cat train df cat train fillna df cat train mode iloc 0 Impute on test df cat test df cat test fillna df cat train mode iloc 0 Impute on train df num train df num train fillna df num train mean Impute on test df num test df num test fillna df num train mean Combine numeric and categorical in train Combine numeric and categorical in test Train Test Train Test scale on train scale on test Create a SVC classifier using a linear kernel Train the classifier Get the best parameters ", "id": "mragpavank/hepatitis-using-svm", "size": "4665", "language": "python", "html_url": "https://www.kaggle.com/code/mragpavank/hepatitis-using-svm", "git_url": "https://www.kaggle.com/code/mragpavank/hepatitis-using-svm"}