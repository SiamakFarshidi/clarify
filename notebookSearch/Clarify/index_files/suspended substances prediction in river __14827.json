{"name": "suspended substances prediction in river ", "full_name": " h1 FE tuning and comparison of the 15 popular models for suspended substances concentration prediction in river water h2 Table of Contents h2 1 Import libraries h2 2 Download datasets h2 3 EDA h2 4 Preparing to modeling h2 5 Tuning models and test for all features h3 5 1 Linear Regression h3 5 2 Support Vector Machines h3 5 3 Linear SVR h3 5 4 MLPRegressor h3 5 5 Stochastic Gradient Descent h3 5 6 Decision Tree Regressor h3 5 7 Random Forest h3 5 8 XGB h3 5 9 LGBM h3 5 10 GradientBoostingRegressor with HyperOpt h3 5 11 RidgeRegressor h3 5 12 BaggingRegressor h3 5 13 ExtraTreesRegressor h3 5 14 AdaBoost Regressor h3 5 15 VotingRegressor h2 6 Models comparison h2 7 Prediction ", "stargazers_count": 0, "forks_count": 0, "description": " a class anchor id 0 a FE tuning and comparison of the 15 popular models for suspended substances concentration prediction in river water Build of the 15 most popular models the most complex models from them are tuned optimized Comparison of the optimal for each type models This kernel is based on the my kernel Titanic 0 83253 Comparison 20 popular models https www kaggle com vbmokin titanic 0 83253 comparison 20 popular models a class anchor id 0 1 a Table of Contents 1 Import libraries 1 1 Download datasets 2 1 EDA 3 1 Preparing to modeling 4 1 Tuning models 5 Linear Regression 5 1 Support Vector Machines 5 2 Linear SVR 5 3 MLPRegressor 5 4 Stochastic Gradient Descent 5 5 Decision Tree Regressor 5 6 Random Forest with GridSearchCV 5 7 XGB 5 8 LGBM 5 9 GradientBoostingRegressor with HyperOpt 5 10 RidgeRegressor 5 11 BaggingRegressor 5 12 ExtraTreesRegressor 5 13 AdaBoost Regressor 5 14 VotingRegressor 5 15 1 Models comparison 6 1 Prediction 7 1 Import libraries a class anchor id 1 a Back to Table of Contents 0 1 2 Download datasets a class anchor id 2 a Back to Table of Contents 0 1 3 EDA a class anchor id 3 a Back to Table of Contents 0 1 This code is based on my kernel FE EDA with Pandas Profiling https www kaggle com vbmokin fe eda with pandas profiling The analysis showed that many values are only available in stations 1 and 2 while others have much less data I propose that at the start code the suspended substances concentration prediction should be carried out only for data from the first two stations 4 Preparing to modeling a class anchor id 4 a Back to Table of Contents 0 1 5 Tuning models and test for all features a class anchor id 5 a Back to Table of Contents 0 1 Thanks to https www kaggle com startupsci titanic data science solutions Now we are ready to train a model and predict the required solution There are 60 predictive modelling algorithms to choose from We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate Our problem is a regression problem We want to identify relationship between output Survived or not with other variables or features Gender Age Port We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset With these two criteria Supervised Learning we can narrow down our choice of models to a few These include Linear Regression Support Vector Machines and Linear SVR Stochastic Gradient Descent GradientBoostingRegressor RidgeCV BaggingRegressor Decision Tree Regressor Random Forest XGBRegressor LGBM ExtraTreesRegressor MLPRegressor Deep Learning VotingRegressor 5 1 Linear Regression a class anchor id 5 1 a Back to Table of Contents 0 1 Linear Regression is a linear approach to modeling the relationship between a scalar response or dependent variable and one or more explanatory variables or independent variables The case of one explanatory variable is called simple linear regression For more than one explanatory variable the process is called multiple linear regression Reference Wikipedia https en wikipedia org wiki Linear regression Note the confidence score generated by the model based on our training dataset 5 2 Support Vector Machines a class anchor id 5 2 a Back to Table of Contents 0 1 Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis Given a set of training samples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier Reference Wikipedia https en wikipedia org wiki Support vector machine 5 3 Linear SVR a class anchor id 5 3 a Back to Table of Contents 0 1 Linear SVR is a similar to SVM method Its also builds on kernel functions but is appropriate for unsupervised learning Reference Wikipedia https en wikipedia org wiki Support vector machine Support vector clustering svr 5 4 MLPRegressor a class anchor id 5 4 a Back to Table of Contents 0 1 The MLPRegressor optimizes the squared loss using LBFGS or stochastic gradient descent by the Multi layer Perceptron regressor Reference Sklearn documentation https scikit learn org stable modules generated sklearn neural network MLPRegressor html sklearn neural network MLPRegressor Thanks to https scikit learn org stable modules generated sklearn neural network MLPRegressor html sklearn neural network MLPRegressor https stackoverflow com questions 44803596 scikit learn mlpregressor performance cap 5 5 Stochastic Gradient Descent a class anchor id 5 5 a Back to Table of Contents 0 1 Stochastic gradient descent often abbreviated SGD is an iterative method for optimizing an objective function with suitable smoothness properties e g differentiable or subdifferentiable It can be regarded as a stochastic approximation of gradient descent optimization since it replaces the actual gradient calculated from the entire data set by an estimate thereof calculated from a randomly selected subset of the data Especially in big data applications this reduces the computational burden achieving faster iterations in trade for a slightly lower convergence rate Reference Wikipedia https en wikipedia org wiki Stochastic gradient descent 5 6 Decision Tree Regressor a class anchor id 5 6 a Back to Table of Contents 0 1 This model uses a Decision Tree as a predictive model which maps features tree branches to conclusions about the target value tree leaves Tree models where the target variable can take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels Decision trees where the target variable can take continuous values typically real numbers are called regression trees Reference Wikipedia https en wikipedia org wiki Decision tree learning 5 7 Random Forest a class anchor id 5 7 a Back to Table of Contents 0 1 Random Forest is one of the most popular model Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n estimators 100 300 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees Reference Wikipedia https en wikipedia org wiki Random forest 5 8 XGB a class anchor id 5 8 a Back to Table of Contents 0 1 XGBoost is an ensemble tree method that apply the principle of boosting weak learners CARTs generally using the gradient descent architecture XGBoost improves upon the base Gradient Boosting Machines GBM framework through systems optimization and algorithmic enhancements Reference Towards Data Science https towardsdatascience com https medium com vishalmorde xgboost algorithm long she may rein edd9f99be63d 5 9 LGBM a class anchor id 5 9 a Back to Table of Contents 0 1 Light GBM is a fast distributed high performance gradient boosting framework based on decision tree algorithms It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf wise So when growing on the same leaf in Light GBM the leaf wise algorithm can reduce more loss than the level wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms Also it is surprisingly very fast hence the word Light Reference Analytics Vidhya https www analyticsvidhya com blog 2017 06 which algorithm takes the crown light gbm vs xgboost 5 10 GradientBoostingRegressor with HyperOpt a class anchor id 5 10 a Back to Table of Contents 0 1 Thanks to https www kaggle com kabure titanic eda model pipeline keras nn Gradient Boosting builds an additive model in a forward stage wise fashion it allows for the optimization of arbitrary differentiable loss functions In each stage n classes regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function Binary classification is a special case where only a single regression tree is induced The features are always randomly permuted at each split Therefore the best found split may vary even with the same training data and max features n features if the improvement of the criterion is identical for several splits enumerated during the search of the best split To obtain a deterministic behaviour during fitting random state has to be fixed Reference sklearn documentation https scikit learn org stable modules generated sklearn ensemble GradientBoostingClassifier html 5 11 RidgeRegressor a class anchor id 5 11 a Back to Table of Contents 0 1 Tikhonov Regularization colloquially known as Ridge Regression is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution This type of problem is very common in machine learning tasks where the best solution must be chosen using limited data If a unique solution exists algorithm will return the optimal value However if multiple solutions exist it may choose any of them Reference Brilliant org https brilliant org wiki ridge regression 5 12 BaggingRegressor a class anchor id 5 12 a Back to Table of Contents 0 1 Bootstrap aggregating also called Bagging is a machine learning ensemble meta algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression It also reduces variance and helps to avoid overfitting Although it is usually applied to decision tree methods it can be used with any type of method Bagging is a special case of the model averaging approach Bagging leads to improvements for unstable procedures which include for example artificial neural networks classification and regression trees and subset selection in linear regression On the other hand it can mildly degrade the performance of stable methods such as K nearest neighbors Reference Wikipedia https en wikipedia org wiki Bootstrap aggregating 5 13 ExtraTreesRegressor a class anchor id 5 13 a Back to Table of Contents 0 1 ExtraTreesRegressor implements a meta estimator that fits a number of randomized decision trees a k a extra trees on various sub samples of the dataset and uses averaging to improve the predictive accuracy and control over fitting The default values for the parameters controlling the size of the trees e g max depth min samples leaf etc lead to fully grown and unpruned trees which can potentially be very large on some data sets To reduce memory consumption the complexity and size of the trees should be controlled by setting those parameter values Reference sklearn documentation https scikit learn org stable modules generated sklearn ensemble ExtraTreesClassifier html In extremely randomized trees randomness goes one step further in the way splits are computed As in random forests a random subset of candidate features is used but instead of looking for the most discriminative thresholds thresholds are drawn at random for each candidate feature and the best of these randomly generated thresholds is picked as the splitting rule This usually allows to reduce the variance of the model a bit more at the expense of a slightly greater increase in bias Reference sklearn documentation https scikit learn org stable modules ensemble html Extremely 20Randomized 20Trees 5 14 AdaBoost Regressor a class anchor id 5 14 a Back to Table of Contents 0 1 The core principle of AdaBoost is to fit a sequence of weak learners i e models that are only slightly better than random guessing such as small decision trees on repeatedly modified versions of the data The predictions from all of them are then combined through a weighted majority vote or sum to produce the final prediction The data modifications at each so called boosting iteration consist of applying N weights to each of the training samples Initially those weights are all set to 1 N so that the first step simply trains a weak learner on the original data For each successive iteration the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data At a given step those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased whereas the weights are decreased for those that were predicted correctly As iterations proceed examples that are difficult to predict receive ever increasing influence Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence Reference sklearn documentation https scikit learn org stable modules ensemble html adaboost 5 15 VotingRegressor a class anchor id 5 15 a Back to Table of Contents 0 1 A Voting Regressor is an ensemble meta estimator that fits base regressors each on the whole dataset It then averages the individual predictions to form a final prediction Reference sklearn documentation https scikit learn org stable modules generated sklearn ensemble VotingRegressor html sklearn ensemble VotingRegressor Thanks for the example of ensemling different models from https scikit learn org stable modules generated sklearn ensemble VotingRegressor html sklearn ensemble VotingRegressor 6 Models comparison a class anchor id 6 a Back to Table of Contents 0 1 We can now compare our models and to choose the best one for our problem Thus the best models are MLPRegressor SGDRegressor and Linear Regression 7 Prediction a class anchor id 7 a Back to Table of Contents 0 1 I hope you find this kernel useful and enjoyable Your comments and feedback are most welcome Go to Top 0  preprocessing models model tuning For boosting model Synthesis valid as test for selection models For models from Sklearn Synthesis valid as test for selection models Relative error between predicted y pred and measured y meas values RMSE between predicted y pred and measured y meas values Calculation of accuracy of boosting model by different metrics Calculation of accuracy of model \u0430\u043a\u0449\u044c Sklearn by different metrics Linear Regression Support Vector Machines Linear SVR MLPRegressor Stochastic Gradient Descent Decision Tree Regressor Random Forest split training set to validation set Gradient Boosting Regressor Ridge Regressor Bagging Regressor Extra Trees Regressor AdaBoost Regressor Plot Plot Plot For models from Sklearn MLPRegressor model for basic train SGDRegressor model for basic train Linear Regressor model for basic train ", "id": "vbmokin/suspended-substances-prediction-in-river", "size": "14827", "language": "python", "html_url": "https://www.kaggle.com/code/vbmokin/suspended-substances-prediction-in-river", "git_url": "https://www.kaggle.com/code/vbmokin/suspended-substances-prediction-in-river"}