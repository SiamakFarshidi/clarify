{"name": "3d convolutional neural network w o programming ", "full_name": " h2 Step 1 Get Access h2 Step 2 Enable Cached Dataset h2 Step 3 Create and Open a New Project h2 Step 4 Select Dataset and do training validation set division h2 Step 5 Build model h2 Step 6 Training and Results h1 3 Pre processing Code h1 Summary ", "stargazers_count": 0, "forks_count": 0, "description": "Welcome everyone to my post that will describe my experiments to get good scores for this problem My aim will be to transfer my knowledge and make it easy for others to follow along Talking about easy we will in fact be building and training our neural networks without doing programming Instead we will use drag and drop GUI based platform Deep Learning Studio to build and train neural network We will try different experiments as we move forward with this competition I will try to documents as much details as I can on this notebook Please feel free to send your suggestions and comments Today we will try 3D Convolutional Neural Network for this problem Full discloure I am one of the cofounder of the company who developed Deep Learning Studio software Deep Learning Studio has a free monthly plan and it offers 2 hours of complementary training time on best GPU available in the Cloud Nvidia K80 with 12GB RAM 1 Pre processing We will do following preprocessing on given CT Scans to make our life easier Code for these steps is mostly borrowed from excellent notebook of Guido Zuidhof Please refer to that Guido Zuidhof notebook to understand these steps in detail Here I will just list major high level preprocessing that we will do on the dataset 1 Load and Convert DICOM file to NUMPY array 2 Do Lung Segmentation on these scans 3 Pad or Trim slices at the end such that every scan has exactly 256 slices 4 Threshold values to below 1100 to 1100 and values above 700 to 700 5 Divide all values with 1100 to bring the range between 1 and 1 You can find full source code for pre processing in section 3 Following experiment uses this preprocessed data as input 2 First Experiment 3D Convolutional Neural Networks Convolutional neural networks have been very successful in image classification and other types of imaging tasks Traditionally convolution neural network operate on a 2D image possibly comprising of 1 or 3 color channels Convolutional networks learns to extract low level features of image automatically This ability comes in handy when tackling with complex real world images You can watch following video to get gentle introduction to convolutional neural network https www youtube com watch v JiN9p5vWHDY ab channel DeepLearning TV Our CT scan dataset is actually comprise of set of slices each slice is 512x512 pixel image We have information if the CT scan contain the cancer or not as a whole Which means that we must process all slices together and then let network correct itself in the end 3D convolutional neural network fit the bill but they tend to consume a lots of GPU memory and are difficult to converge But let s make a network and give it a shot Step 1 Get Access Sign up and get access to Deep Learning Studio at http deepcognition ai 1 Step 2 Enable Cached Dataset Enable cached dataset in your account by uploading two small files that you must download from your Kaggle account These files must be uploaded for to verify that user is infact has access to Kaggle dataset Follow markers 1 to 4 Enable Access to Cached Dataset 2 Step 3 Create and Open a New Project Let s build a new project by going to project menu on left and clicking on button enter image description here 3 Give a name and description to your project Now open the project by clicking on box arrow icon on project bar Open Project 4 Step 4 Select Dataset and do training validation set division We will do training with 1200 samples and we will use 197 samples for validation for this example Training and Validation Split 5 Step 5 Build model Once dataset is selected click on Model Tab and start building model as shown below by dragging layers from left menu bar to the canvas and connecting these layer blocks Architecture 6 You will also need to set the parameters of the layers Below is the actual generated source code using view code button in Model tab for the model that I built and you can reference it to get parameter values def get model Input 1 Input shape 256 512 512 1 MaxPooling3D 27 MaxPooling3D pool size 1 3 3 Input 1 Convolution3D 1 Convolution3D kernel dim1 4 nb filter 10 activation relu kernel dim3 4 kernel dim2 4 MaxPooling3D 27 Convolution3D 7 Convolution3D kernel dim1 4 nb filter 10 activation relu kernel dim3 4 kernel dim2 4 Convolution3D 1 BatchNormalization 28 BatchNormalization Convolution3D 7 MaxPooling3D 12 MaxPooling3D pool size 2 2 2 BatchNormalization 28 SpatialDropout3D 1 SpatialDropout3D p 0 5 MaxPooling3D 12 Convolution3D 9 Convolution3D kernel dim1 2 nb filter 20 activation relu kernel dim3 2 kernel dim2 2 SpatialDropout3D 1 Convolution3D 11 Convolution3D kernel dim1 2 nb filter 20 activation relu kernel dim3 2 kernel dim2 2 Convolution3D 9 BatchNormalization 9 BatchNormalization Convolution3D 11 MaxPooling3D 14 MaxPooling3D pool size 2 2 2 BatchNormalization 9 SpatialDropout3D 4 SpatialDropout3D p 0 5 MaxPooling3D 14 Convolution3D 12 Convolution3D kernel dim1 2 nb filter 40 activation relu kernel dim3 2 kernel dim2 2 SpatialDropout3D 4 Convolution3D 13 Convolution3D kernel dim1 2 nb filter 40 activation relu kernel dim3 2 kernel dim2 2 Convolution3D 12 MaxPooling3D 23 MaxPooling3D pool size 2 2 2 Convolution3D 13 BatchNormalization 23 BatchNormalization MaxPooling3D 23 SpatialDropout3D 5 SpatialDropout3D p 0 5 BatchNormalization 23 GlobalMaxPooling3D 1 GlobalMaxPooling3D SpatialDropout3D 5 Dense 1 Dense activation relu output dim 10 GlobalMaxPooling3D 1 Dropout 14 Dropout p 0 3 Dense 1 Dense 6 Dense activation relu output dim 10 Dropout 14 Dense 2 Dense activation softmax output dim 2 Dense 6 return Model Input 1 Dense 2 Rationale for this architecture First MaxPooling3D layer is done to reduce size of the scan kind of downscaling because even the GPUs like K80 with 12GB RAM are not able to fit this scan with reasonable model in memory Our architecture is based on stacking multiple blocks of following Conv3D Conv3D BatchNorm MaxPooling3D SpatialDropout3D Purpose of first two Conv3D layers is to extract features from input BatchNormalization layer is added to accelerate the training see https arxiv org abs 1502 03167 MaxPool is added to reduce spacial dimensions for future blocks SpacialDropout3D is added added to make system more robust and less prone to over fitting At the end of convolutional network we do Global max pooling to pool the features which then go into three dense layers to bring the final dimension to 2 which is the size of our output label cancer or no cancer Note that by no mean this is the best architecture but I wanted to share my experiment with you guys in the hope it can help you build even better network Your suggestions are welcome Step 6 Training and Results Now you can go to Hyperparameters tab and make sure batch size is set to 1 This is important because anything bigger will not fit GPUs memory and training will fail Hyperparameters 7 Finally you can move to Training tab Select GPU K80 as instance and click on Start Instance Once Instance has been started Click on Start Training Note that training is going to be very slow because of sheer size of dataset and computations needed After trying out 2 epochs I was able to get loss of about 0 58 on validation set Training Dashboard 8 1 http deepcognition ai 2 https s3 us west 2 amazonaws com deepcognition 3dconvnet cached dataset jpg 3 https s3 us west 2 amazonaws com deepcognition 3dconvnet new project jpg 4 https s3 us west 2 amazonaws com deepcognition 3dconvnet open project jpg 5 https s3 us west 2 amazonaws com deepcognition 3dconvnet dataset selection jpg 6 https s3 us west 2 amazonaws com deepcognition 3dconvnet model jpg 7 https s3 us west 2 amazonaws com deepcognition 3dconvnet hyperparameters jpg 8 https s3 us west 2 amazonaws com deepcognition 3dconvnet training dashboard jpg 3 Pre processing Code This notebook converts DICOM scans to Numpy array along with doing segmentation normalization etc 1 It allows to use multi CPU to do segmentation 2 slow slice function is designed to show scan at full resolution Basic imshow only shows scaled version of scan Summary In this post we built a working convolutional 3D neural network without programming Please feel free to modify and experiment with it Currently I am working on following two more appoarches 1 Reduce dimensionality of scans using autoencoders to make it easy to process the dataset using some other neural network 2 Use Convolutional LSTM neural network that combines both CNN and LSTM for analyzing sequence of images I hope to share more details about these experiments with you in coming days If you liked this post please give it a upvote Thank you  Please check excellent notebook of Guido Zuidhof for full explanation of this code For Kaggle I have added sample image directory only fix random seed for reproducibility Simple Function to show the slice at full resolustion normal imshow would downscale this image It can accept either image width image height array or image width image height 1 numpy as input Optional Value range is a tuple of fixed max value and min value This is useful if you do not want color to change between different scan slices 5 of the width height of the figure Make a figure big enough to accomodate an axis of xpixels by ypixels as well as the ticklabels etc Make the axis the right size For Testing feed just load one scan Multi threaded processes to utilize all available CPUs for this task Note that many threads will block on IO so creating more than number of CPUs Cleanup For demo reduce number of slices to 5 to save time For Convnet we will need one extra dimension representing color channel Save output file to compressed npz file for easy reading Load the scans in given folder path Convert to int16 from sometimes int16 should be possible as values should always be low enough 32k Set outside of scan pixels to 0 The intercept is usually 1024 so air is approximately 0 Convert to Hounsfield units HU make total 256 slices fill in 1100 as exterme value Ignore all slices later than 255 if required Creation of the internal Marker Creation of the external Marker Creation of the Watershed Marker matrix Creation of the Sobel Gradient Watershed algorithm Reducing the image created by the Watershed algorithm to its outline Performing Black Tophat Morphology for reinclusion Creation of the disk kernel and increasing its size a bit Perform the Black Hat Use the internal marker and the Outline that was just created to generate the lungfilter Close holes in the lungfilter fill holes is not used here since in some slices the heart would be reincluded by accident Apply the lungfilter note the filtered areas being assigned 30 HU Maximum absolute value of any pixel This will bring values between 1 and 1 For full preprocessing you should to set demo False ", "id": "deepman/3d-convolutional-neural-network-w-o-programming", "size": "9613", "language": "python", "html_url": "https://www.kaggle.com/code/deepman/3d-convolutional-neural-network-w-o-programming", "git_url": "https://www.kaggle.com/code/deepman/3d-convolutional-neural-network-w-o-programming"}