{"name": "breast cancer prediction 99 ", "full_name": " h1 Data Set Information h2 Attribute Information h1 1 import library h1 2 load and analys data h4 from the first look in our data description we can see that h2 A finding missing values h1 3 feautres selection h4 Diagnosis h2 A correlation map h4 this feautres had a corralation valus 0 07 with the target columns h1 4 data vizualisation h2 Plotting many distributions h2 A diagnosis h2 B concave points worst h2 B concavity mean h2 3 perimeter worst h1 5 machine learning application h2 A split data h2 B Feature Scaling h3 a Logistic Regression h3 b KNN h3 c Support Vector Machines h3 d DecisionTreeClassifier h3 e RANDOM FOREST CLASSIFCATION h3 f ANN neural network h3 g xgboost h3 h catboost ", "stargazers_count": 0, "forks_count": 0, "description": " Data Set Information Features are computed from a digitized image of a fine needle aspirate FNA of a breast mass They describe characteristics of the cell nuclei present in the image A few of the images can be found at Web Link Separating plane described above was obtained using Multisurface Method Tree MSM T K P Bennett Decision Tree Construction Via Linear Programming Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society pp 97 101 1992 a classification method which uses linear programming to construct a decision tree Relevant features were selected using an exhaustive search in the space of 1 4 features and 1 3 separating planes The actual linear program used to obtain the separating plane in the 3 dimensional space is that described in K P Bennett and O L Mangasarian Robust Linear Programming Discrimination of Two Linearly Inseparable Sets Optimization Methods and Software 1 1992 23 34 This database is also available through the UW CS ftp server ftp ftp cs wisc edu cd math prog cpo dataset machine learn WDBC Attribute Information 1 ID number 2 Diagnosis M malignant B benign 3 3 32 Ten real valued features are computed for each cell nucleus radius mean of distances from center to points on the perimeter texture standard deviation of gray scale values perimeter area smoothness local variation in radius lengths compactness perimeter 2 area 1 0 concavity severity of concave portions of the contour concave points number of concave portions of the contour symmetry fractal dimension coastline approximation 1 https static packt cdn com products 9781783980284 graphics 3a298fcc 54fb 42c2 a212 52823e709e30 png 1 import library 2 load and analys data like we see all our feautres are numirical values exept the target value diagnosis M malignant B benign we had 569 Rows and 33 columns small data from the first look in our data description we can see that 1 B benign is the most frequent value in our target columns 2 Unnamed 32 columns is an empty column A finding missing values like we c our data is clean exept the last columns that is empty so we gonna drop it 3 feautres selection Diagnosis 1 M malignant 1 2 B benign 0 A correlation map this feautres had a corralation valus 0 07 with the target columns fractal dimension mean texture se smoothness se symmetry se fractal dimension se 4 data vizualisation Plotting many distributions A diagnosis B concave points worst B concavity mean We can see there are some outliers Lets remove them 0 3 0 5 3 perimeter worst We can see there are 2 outliers Lets remove them 120 5 machine learning application A split data B Feature Scaling a Logistic Regression b KNN c Support Vector Machines d DecisionTreeClassifier e RANDOM FOREST CLASSIFCATION f ANN neural network g xgboost h catboost  manipulation data visualiation data default theme drop the id columns transformation of type of the target value to numerical drop this columns Feature Selection Making Confusion Matrix and calculating accuracy score Fit the model Confusion Matrix accuracy score Finding the optimum number of neighbors Training the K Nearest Neighbor Classifier on the Training set Predicting the Test set results Making the confusion matrix and calculating accuracy score Training the Support Vector Classifier on the Training set Predicting the test set results Making the confusion matrix and calculating accuracy score Finding the optimum number of max leaf nodes Training the Decision Tree Classifier on the Training set Predicting the test set results Making the confusion matrix and calculating accuracy score Finding the optimum number of n estimators Training the RandomForest Classifier on the Training set Predicting the test set results Making the confusion matrix and calculating the accuracy score Initialising the ANN Adding the input layer and the first hidden layer Adding the second hidden layer Adding the third hidden layer Adding the fourth hidden layer Adding the output layer Compiling the ANN Training the ANN on the training set Predicting the test set results Making the confusion matrix calculating accuracy score confusion matrix accuracy Making the confusion matrix and calculating the accuracy score Making the confusion matrix and calculating the accuracy score ", "id": "midouazerty/breast-cancer-prediction-99", "size": "3103", "language": "python", "html_url": "https://www.kaggle.com/code/midouazerty/breast-cancer-prediction-99", "git_url": "https://www.kaggle.com/code/midouazerty/breast-cancer-prediction-99"}