{"name": "modeling energy efficiency residential building ", "full_name": " h1 Introduction h1 1 Introduction h1 2 Data Preparation h1 A Data import h1 Variable s Information h1 B Spliting the data in X and Y h1 3 Fitting modeling h1 4 Models parameters tuning h1 A Decision Tree Regressor parameters turning h1 Parameters h1 B Tune Random Forests Parameters h1 C Gradient Boosting Regression Hyperparameter Tuning h1 D CatBoostRegressor h1 E And suprising MLPRegressor h1 3 CONCLUSION ", "stargazers_count": 0, "forks_count": 0, "description": " a href https ibb co tc0YwXc img src https i ibb co 3WxS5zW Energy Efficiency Buildings jpg alt Energy Efficiency Buildings border 0 a Introduction Buildings energy consumption is put away around 40 of total energy use Predicting heating and cooling loads of a building in the initial phase of the design to find out optimal solutions amongst different designs is very important as well as in the operating phase after the building has been finished for efficient energy In this Notebook diferent models were applied for predicting heating and cooling loads of a building based on a dataset for building energy performance Input variables are relative compactness roof area overall height surface area glazing are a wall area glazing area distribution of a building orientation Output variables heating loads and cooling loads of the building The model was trained and validated on 33 on the data set and the accuracy for the prediction test was 99 9 and 99 6 respectively 1 Introduction In a building thermal energy involves two measures of cooling load CL and heating load HL and these measures are regulated by heating ventilation and air conditioning HVAC system The HVAC system is designed to compute the HL and CL of the space and thereby provide a desirable indoor air condition In this sense some studies have focused on evaluating comfortable yet energy saving spaces Required cooling and heating capacities are estimated mainly according to the basic factors including building properties its utilization and climate conditions More sustainable consumption of energy can be ensured by a proper examination of the energy performance of buildings EPB and optimal designing of the HVAC system Although many countries take such measures there is still a high level of energy consumption and it is projected to increase globally According to the items mentioned above many engineers have tried to develop different predictive and evaluative tools the primary aim in order to produce an optimal approximation of building energy consumption 2 Data Preparation In this Notebook we used the dataset taken from https cml ics uci edu based on research by Tsanas and Xifara A Data import Variable s Information Relative Compactness Surface Area m\u00b2 Wall Area m\u00b2 Roof Area m\u00b2 Overall Height m Orientation 2 North 3 East 4 South 5 West Glazing Area 0 10 25 40 of floor area Glazing Area Distribution Variance 1 Uniform 2 North 3 East 4 South 5 West Heating Load kWh Cooling Load kWh Good I m lucky since not have to process the NaN data Now check the distribution of different variables And try to check the correlation between variables Not easy to find out a clear correlation we tried to check the correlation matrix OK it s not bad Let s begin to process the data for fitting modeling B Spliting the data in X and Y Importing necessary Libraries Set variables target functions Splitting the dataset into Training and Test set Feature scaling or data normalization is a method used to normalize the range of independent variables or features of data So when the values vary a lot in an independent variable we use feature scaling so that all the values remain in the comparable range 3 Fitting modeling Create a DataFrame to store computation results obtained with different models Now let s try to select some Regressors to check its performance Wow very impressive with the GradientBoostingRegressor 4 Models parameters tuning Boosting machine learning algorithms are highly used because they give better accuracy over simple ones Performance of these algorithms depends on hyperparameters An optimal set of parameters can help to achieve higher accuracy Finding hyperparameters manually is tedious and computationally expensive Therefore automation of hyperparameters tuning is important RandomSearch GridSearchCV and Bayesian optimization are generally used to optimize hyperparameters In this Notebook we calculate the best parameters for the model using GridSearchCV A Decision Tree Regressor parameters turning Decision Tree algorithm has become one of the most used machine learning algorithm both in competitions like Kaggle as well as in business environment Decision Tree can be used both in classification and regression problem The model is based on decision rules extracted from the training data In regression problem the model uses the value instead of class and mean squared error is used to for a decision accuracy Decision tree model is not good in generalization and sensitive to the changes in training data A small change in a training dataset may effect the model predictive accuracy Parameters max features The number of randomly chosen features from which to pick the best feature to split on a given tree node It can be an integer or one of the two following methods auto square root of the total number of predictors max number of predictors max leaf nodes The maximum number of leaf nodes a tree in the forest can have an integer between 1 and 1e9 inclusive max depth The maximum depth for growing each tree an integer between 1 and 100 inclusive min samples leaf The minimum number of samples each branch must have after splitting a node an integer between 1 and 1e6 inclusive A split that causes fewer remaining samples is discarded As observed in the fitting calculation section we will try to tuning model parameters using the training data set of Cooling load or y2 train Yes the accuracy of the model has been improved B Tune Random Forests Parameters Random forest is an ensemble tool which takes a subset of observations and a subset of variables to build a decision trees It builds multiple such decision tree and amalgamate them together to get a more accurate and stable prediction We generally see a random forest as a black box which takes in input and gives out predictions without worrying too much about what calculations are going on the back end This black box itself have a few levers we can play with Each of these levers have some effect on either the performance of the model or the resource time balance Parameters in random forest are either to increase the predictive power of the model or to make it easier to train the model There are primarily 3 features which can be tuned to improve the predictive power of the model Max features These are the maximum number of features Random Forest is allowed to try in individual tree Increasing max features generally improves the performance of the model as at each node now we have a higher number of options to be considered However this is not necessarily true as this decreases the diversity of individual tree which is the USP of random forest But for sure you decrease the speed of algorithm by increasing the max features Hence you need to strike the right balance and choose the optimal max features n estimators This is the number of trees you want to build before taking the maximum voting or averages of predictions Higher number of trees give you better performance but makes your code slower You should choose as high value as your processor can handle because this makes your predictions stronger and more stable min sample leaf If you have built a decision tree before you can appreciate the importance of minimum sample leaf size Leaf is the end node of a decision tree A smaller leaf makes the model more prone to capturing noise in train data Generally I prefer a minimum leaf size of more than 50 However you should try multiple leaf sizes to find the most optimum for your use case There are a few attributes which have a direct impact on model training speed In this Notebook we just mention about the n jobs parameter This parameter tells the engine how many processors is it allowed to use A value of 1 means there is no restriction whereas a value of 1 means it can only use one processor Just improved a little bit accuracy C Gradient Boosting Regression Hyperparameter Tuning What we will do now is make an instance of the GradientBoostingRegressor We will create our grid with the various values for the hyperparameters We will then take this grid and place it inside GridSearchCV function so that we can prepare to run our model There are some arguments that need to be set inside the GridSearchCV function such as estimator grid cv etc With this tuning we can see that the mean squared error is lower than with the baseline model We can now move to the final step of taking these hyperparameter settings and see how they do on the dataset There are several hyperparameters we need to tune and they are as follows Learning rate The learning rate is the weight that each tree has on the final prediction Number of estimators The number of estimators is show many trees to create The more trees the more likely to overfit Min samples split The minimum number of samples required to split an internal node Max depth Maximum depth of the individual regression estimators The maximum depth limits the number of nodes in the tree Tune this parameter for best performance the best value depends on the interaction of the input variables Subsample Subsample is the proportion of the sample to use Wow we are happy with this improvement then D CatBoostRegressor CatBoost is a recently open sourced machine learning algorithm from Yandex It can work with diverse data types to help solve a wide range of problems that businesses face today To top it up it provides best in class accuracy It yields state of the art results without extensive data training typically required by other machine learning methods and Provides powerful out of the box support for the more descriptive data formats that accompany many business problems CatBoost name comes from two words Category and Boosting Boost comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection recommendation items forecasting and it performs well also It can also return very good result with relatively less data unlike DL models that need to learn from a massive amount of data CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front Handling Categorical features automatically We can use CatBoost without any explicit pre processing to convert categories into numbers CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features It also reduces the need for extensive hyper parameter tuning and lower the chances of overfitting also which leads to more generalized models Although CatBoost has multiple parameters to tune and it contains parameters like the number of trees learning rate regularization tree depth fold size bagging temperature and others iterations The maximum number of trees that can be built when solving machine learning problems Fewer may be used learning rate used for reducing the gradient step It affects the overall time of training the smaller the value the more iterations are required for training depth Depth of the tree Can be any integer up to 32 Good values in the range 1 10 Parameter Search is usually known as grid search basically looking for parameter values that score the best An important thing to remember is that we can t use the test set to tune parameters otherwise we ll overfit to the test set Preventing Overfitting CatBoost provides a nice facility to prevent overfitting If you set iterations to be high the classifier will use many trees to build the final classifier and you risk overfitting If you set use best model True and eval metric Accuracy when initialising and then set eval set to be a validation set then CatBoost won t use all the iterations it will return the iteration that gives the best accuracy on the eval set This is similar to early stopping used in neural networks If you are having problems with overfitting it would be a good idea to try this I didn t see any improvement on this dataset though probably because there are so many train points it is more difficult to overfit on E And suprising MLPRegressor Now it s time to summary all our BEST models 3 CONCLUSION In this Notebook building energy performance has been investigated using different models to predict Heating and Cooling loads We tried to learn how to tune different parameters in the models and obtained a very good prediction result 99 5 on both Heating and Cooling loads compared to the experimental data set Some observations will be shown in the graphs bellow  This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e g pd read csv Input data files are available in the read only input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 20GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session ", "id": "winternguyen/modeling-energy-efficiency-residential-building", "size": "13126", "language": "python", "html_url": "https://www.kaggle.com/code/winternguyen/modeling-energy-efficiency-residential-building", "git_url": "https://www.kaggle.com/code/winternguyen/modeling-energy-efficiency-residential-building"}