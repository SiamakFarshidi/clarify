{"name": "heart desease ml classificationexercises ", "full_name": " h2 If you find this kernel helpful Please UPVOTES h2 Problem Definition h2 Data contains h2 READ DATA AND EXPLORING DATA h2 SOME VISUALIZATION h3 Count of disease and not desease h3 Distribution of disease and not disease with scatter h3 Distrbution of age with distplot h3 Distribution of age with boxplot h3 Dividing into age groups h3 Dividing into age groups with barplot h3 Dividing into age groups with pieplot h3 Distrubution of Age and Target with violinplot h3 sex and ca hue target with barplot h3 Sex and Oldpeak hue restecg h3 Count of target with hue sex h3 Number of people who have heart disease according to age h2 Correlation matrix heatmap h3 Interpretation of heatmap h3 correlation only with target and other variables h2 Target and Thalech h3 Interpretation h4 CONCLUSION OF VISUALIZATION h2 LETS NORMALIZE THE VARIABLES h3 Normalization h1 LETS TRY CLASSIFICATIONS METHODS h2 1 LOGISTIC REGRESSION h3 A Train test splitting h3 B Modeling of Logistic R Method h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h2 2 NAIVE BAYES METHOD h3 A Train test splitting h3 B Modeling of Naive B Method h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h2 3 KNN METHOD h3 A Train test splitting h3 B Modeling of KNN Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 Look at accuracy score h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h3 Conclusion KNN h2 4 SVM SUPPORT VECTOR MACHINES h3 A Train test splitting h3 B Modeling of SVM Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h3 E TUNING THE PREDICTION WE can tune our prediction h4 Tuning1 change C and gamma h4 Tuning2 changing kernel linear c 100 h3 Conclusion h2 5 RANDOM FOREST METHOD h3 A Train test splitting h3 B Modeling of SVM Medhod h3 C Lets control the succes score prediction accuracy score confusion m on test data h4 HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap h4 CLASSICICATION REPORT we can also see classification report h4 LOOK AT ALL PREDICTION VALUE ON TEST DATA h3 E TUNING THE PREDICTION WE can tune our prediction h3 Lets look at importance 6 variables h4 Conclusion h2 6 DECISION TREE METHOD h3 A Train test splitting h3 B Modeling of Decision Tree h3 C Lets control the succes score prediction accuracy score confusion m on test data h3 D Model tuning h3 Lets look at 6 importance variables h2 LETS SEE ALL SCORE OF CLASSIFICATIONS METHODS h2 FINISH ", "stargazers_count": 0, "forks_count": 0, "description": " If you find this kernel helpful Please UPVOTES Problem Definition Given clinical parameters about a patient can we predict whether or not they have heart disease Data contains age age in years sex 1 male 0 female cp chest pain type gogus agrisi tipi trestbps resting blood pressure in mm Hg on admission to the hospital kan basinci chol serum cholestoral in mg dl mg dl cinsinden serum kolesterol\u00fc fbs fasting blood sugar 120 mg dl 1 true 0 false restecg resting electrocardiographic results dinlenme elektrokardiyografik sonu\u00e7lar\u0131 thalach maximum heart rate achieved ula\u015f\u0131lan maksimum kalp at\u0131\u015f h\u0131z\u0131 exang exercise induced angina 1 yes 0 no egzersize ba\u011fl\u0131 anjina 1 evet 0 hay\u0131r oldpeak ST depression induced by exercise relative to rest dinlenmeye g\u00f6re egzersizin neden oldu\u011fu ST depresyonu slope the slope of the peak exercise ST segment en y\u00fcksek egzersiz ST segmentinin e\u011fimi ca number of major vessels 0 3 colored by flourosopy thal 3 normal 6 fixed defect 7 reversable defect target have disease or not 1 yes 0 no hastal\u0131\u011f\u0131 var m\u0131 yok mu 1 evet 0 hay\u0131r READ DATA AND EXPLORING DATA SOME VISUALIZATION Count of disease and not desease Distribution of disease and not disease with scatter Distrbution of age with distplot Distribution of age with boxplot Dividing into age groups Dividing into age groups with barplot There are a few young ages Dividing into age groups with pieplot Distrubution of Age and Target with violinplot sex and ca hue target with barplot Sex and Oldpeak hue restecg Count of target with hue sex Number of people who have heart disease according to age Correlation matrix heatmap Interpretation of heatmap From the above correlation heat map we can conclude that target and cp variable are mildly positively correlated correlation coefficient 0 43 target and thalach variable are also mildly positively correlated correlation coefficient 0 42 target and slope variable are weakly positively correlated correlation coefficient 0 35 target and exang variable are mildly negatively correlated correlation coefficient 0 44 target and oldpeak variable are also mildly negatively correlated correlation coefficient 0 43 target and ca variable are weakly negatively correlated correlation coefficient 0 39 target and thal variable are also waekly negatively correlated correlation coefficient 0 34 correlation only with target and other variables Target and Thalech Interpretation We can see that those people suffering from heart disease target 1 have relatively higher heart rate thalach as compared to people who are not suffering from heart disease target 0 CONCLUSION OF VISUALIZATION Findings of Bivariate Analysis are as follows There is no variable which has strong positive correlation with target variable There is no variable which has strong negative correlation with target variable There is no correlation between target and fbs The cp and thalach variables are mildly positively correlated with target variable We can see that the thalach variable is slightly negatively skewed The people suffering from heart disease target 1 have relatively higher heart rate thalach as compared to people who are not suffering from heart disease target 0 LETS NORMALIZE THE VARIABLES Normalization LETS TRY CLASSIFICATIONS METHODS Now we ve got our data split into training and test sets it s time to build a machine learning model We ll train it find the patterns on the training set And we ll test it use the patterns on the test set We re going to try machine learning models 1 Logistic Regression 2 K Nearest Neighbours Classifier 3 Support Vector machine 4 Decision Tree Classifier 5 Random Forest Classifier 1 LOGISTIC REGRESSION Logistic Regression is a useful model to run early in the workflow Logistic regression measures the relationship between the categorical dependent variable feature and one or more independent variables features by estimating probabilities using a logistic function which is the cumulative logistic distribution Lojistik regresyon k\u00fcm\u00fclatif lojistik da\u011f\u0131l\u0131m olan bir lojistik fonksiyon kullanarak olas\u0131l\u0131klar\u0131 tahmin ederek kategorik ba\u011f\u0131ml\u0131 de\u011fi\u015fken \u00f6zellik ile bir veya daha fazla ba\u011f\u0131ms\u0131z de\u011fi\u015fken \u00f6zellik aras\u0131ndaki ili\u015fkiyi \u00f6l\u00e7er A Train test splitting B Modeling of Logistic R Method C Lets control the succes score prediction accuracy score confusion m on test data CLASSICICATION REPORT we can also see classification report LOOK AT ALL PREDICTION VALUE ON TEST DATA E TUNING THE PREDICTION WE can tune our prediction We can see If we change our condition for probobilty our prediction and confusion matrix and accuracy score change 2 NAIVE BAYES METHOD In machine learning Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong naive independence assumptions between the features Naive Bayes classifiers are highly scalable requiring a number of parameters linear in the number of variables features in a learning problem A Train test splitting B Modeling of Naive B Method C Lets control the succes score prediction accuracy score confusion m on test data HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap CLASSICICATION REPORT we can also see classification report LOOK AT ALL PREDICTION VALUE ON TEST DATA E TUNING THE PREDICTION WE can tune our prediction If we tune our data for nb it increase a little nb tuned bestscore 89 and cmnb best are our best best score and our best confusion matrix 3 KNN METHOD In pattern recognition the k Nearest Neighbors algorithm or k NN for short is a non parametric method used for classification and regression A sample is classified by a majority vote of its neighbors with the sample being assigned to the class most common among its k nearest neighbors k is a positive integer typically small In this method we need to choose k value It means that we chose k number of points of classes which are nearest to the out test point We can call this small data set We count the number of classes in the small dataset and determine the highest number of class Finally we can say our test point belongs to the class While choosing k number we have to be carefull because small k value causes overfitting while big k value causes underfitting Coding is the same for all supervised classes and we jus need to change the last part of the code K 1 SECERSEK OVERFITTING OLABILIR K BUYUK SECERSEK UNDERFITTING OLABILIR A Train test splitting B Modeling of KNN Medhod C Lets control the succes score prediction accuracy score confusion m on test data Look at accuracy score HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap CLASSICICATION REPORT we can also see classification report LOOK AT ALL PREDICTION VALUE ON TEST DATA E TUNING THE PREDICTION WE can tune our prediction we can tune n neigbors metric If we use n neighbors 21 we can obtain best score we tune the knn than our score increase If we change metric and use tuned n neigbors acurracy score is best There are many kinds of metric in KNN minkowski hamming Conclusion KNN knn tuned bestscore 85 and cmknn best are our best best score and our best confusion matrix 4 SVM SUPPORT VECTOR MACHINES Support Vector Machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis Given a set of training samples each marked as belonging to one or the other of two categories an SVM training algorithm builds a model that assigns new test samples to one category or the other making it a non probabilistic binary linear classifier SVM is used fo both regression and classification problems but generally for classification There is a C parameter inside the SVM algoritma and the default value of C parameter is 1 If C is small it causes the misclassification If C is big it causes ovetfitting So we need to try C parameter to find best value SVM hem regresyon hem de s\u0131n\u0131fland\u0131rma problemleri i\u00e7in kullan\u0131l\u0131r ancak genellikle s\u0131n\u0131fland\u0131rma i\u00e7in kullan\u0131l\u0131r SVM i\u00e7erisinde C parametresi vard\u0131r ve C parametresinin default de\u011feri 1 dir C nin k\u00fc\u00e7\u00fck olmas\u0131 yanl\u0131\u015f s\u0131n\u0131fland\u0131rmaya neden olur C b\u00fcy\u00fckse overfitting e neden olur Bu y\u00fczden en iyi de\u011feri bulmak i\u00e7in C parametresini denememiz gerekiyor A Train test splitting B Modeling of SVM Medhod C Lets control the succes score prediction accuracy score confusion m on test data HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap CLASSICICATION REPORT we can also see classification report E TUNING THE PREDICTION WE can tune our prediction Look at c kernel gamma Tuning1 change C and gamma Tuning2 changing kernel linear c 100 Tuning3 changing kernel rbf c 100 Conclusion svm score1 84 is the best score and c svm is the best confusion matrix 5 RANDOM FOREST METHOD Random Forests is one of the most popular model Random forests or random decision forests are an ensemble learning method for classification regression and other tasks that operate by constructing a multitude of decision trees n estimators 100 300 at training time and outputting the class that is the mode of the classes classification or mean prediction regression of the individual trees This methods basically use multiple number of decision trees and take the avarage of the results of these decision trees And we use this avarage to determine the class of the test point This is one of ensamble method which uses multiple classes to predict the target and very powerfull technique A Train test splitting B Modeling of SVM Medhod C Lets control the succes score prediction accuracy score confusion m on test data HEATMAP IN CONFUSION MATRIX We can see the confusion matrix in Heatmap CLASSICICATION REPORT we can also see classification report LOOK AT ALL PREDICTION VALUE ON TEST DATA E TUNING THE PREDICTION WE can tune our prediction n estimators importance variables Lets look at importance 6 variables Conclusion rf2 score 84 is the best score and c rf2 is the best confusion matrix 6 DECISION TREE METHOD This model uses a Decision Tree as a predictive model which maps features tree branches to conclusions about the target value tree leaves Tree models where the target variable can take a finite set of values are called classification trees in these tree structures leaves represent class labels and branches represent conjunctions of features that lead to those class labels Decision trees where the target variable can take continuous values typically real numbers are called regression trees Decision tree builds classification or regression models in the form of a tree structure It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed According to \u0131nformation entropy we can determine which feature is the most important And we put the most important one to the top of the related tree Decision tree classification can be used for both binary and multi classes Coding is the same for all supervised classes and we jus need to change the last part of the code A Train test splitting B Modeling of Decision Tree C Lets control the succes score prediction accuracy score confusion m on test data D Model tuning Lets look at 6 importance variables LETS SEE ALL SCORE OF CLASSIFICATIONS METHODS FINISH  This Python 3 environment comes with many helpful analytics libraries installed It is defined by the kaggle python Docker image https github com kaggle docker python For example here s several helpful packages to load linear algebra data processing CSV file I O e g pd read csv For data visualization Plotly for interactive graphics Disabling warnings Input data files are available in the read only input directory For example running this by clicking run or pressing Shift Enter will list all files under the input directory You can write up to 5GB to the current directory kaggle working that gets preserved as output when you create a version using Save Run All You can also write temporary files to kaggle temp but they won t be saved outside of the current session chose randon sample from row DISTRUBUTION OF AGE WITH DISTPLOT DISTRUBUTION OF AGE WITH BOXPLOT we can see in pie Number of people who have heart disease according to age Let s make our correlation matrix a little prettier with jitter with boxplot We can see what there is in lr icinde hangi secenekler vargormek icin LogisticRegression C 1 0 class weight None dual False fit intercept True intercept scaling 1 l1 ratio None max iter 100 multi class auto n jobs None penalty l2 random state None solver liblinear tol 0 0001 verbose 0 warm start False sabit katsayi degisken katsayilari The y predicted by the y in the test are compared test deki y ile tahmin edilen yler karsilastiriliyor Dogru tahmin etme yuzdesi bulunuyor We found the numbers of guessing with confusion matrix 31 for 1 correct guess 0 for 35 correct guess The top was imported confusion matrixle tahmin etme sayilarini bulduk 1 icin 31 i dogru tahmin 0 icin 35 i dogru tahmin En ustte import edildi Hepsi icin yapilabilir ilk 10 datatest deki tahminlerimiz 1 si 0 olma 2 si 1 olma olasiligi oranlari we can look at which option is there in GaussionNB nb confusion matrixle tahmin etme sayilarini bulduk 1 icin 32 i dogru tahmin 0 icin 30 i dogru tahmin En ustte import edildi Hepsi icin yapilabilir ilk 10 datatest deki tahminlerimiz 1 si 0 olma 2 si 1 olma olasiligi oranlari KNeighborsClassifier algorithm auto leaf size 30 metric minkowski metric params None n jobs None n neighbors 3 p 2 weights uniform Hepsi icin yapilabilir ilk 10 datatest deki tahminlerimiz 1 si 0 olma 2 si 1 olma olasiligi oranlari ERROR ON TRAIN DATA We use Grid for tuning we obta cross validation yontemi kullaniliyor nesnesi tanimlandi Hepsi icin yapilabilir svm SVC C 5 break ties False cache size 200 class weight None coef0 0 0 decision function shape ovr degree 9 gamma scale kernel poly max iter 1 probability False random state None shrinking True tol 0 001 verbose False Hepsi icin yapilabilir EN UYGUN C VE GAMMA DEGERI BULMA svm tune1 SVC C 100 gamma 0 0001 degree 9 kernel poly svm tune1 fit x train y train y pred svm predict x test cok uzun suruyor we changed the kernel We can use linear poly rbf we changed the kernel We can use linear poly rbf svc tuned SVC C 100 gamma 0 0001 kernel linear svc tuned fit x train y train y pred svc tuned predict x test accuracy score y test y pred uzun suruyor RandomForestClassifier bootstrap True ccp alpha 0 0 class weight None criterion gini max depth None max features auto max leaf nodes None max samples None min impurity decrease 0 0 min impurity split None min samples leaf 1 min samples split 2 min weight fraction leaf 0 0 n estimators 100 n jobs None oob score False random state None verbose 0 warm start False Hepsi icin yapilabilir yukarda import edildi ilk 10 datatest deki tahminlerimiz 1 si 0 olma 2 si 1 olma olasiligi oranlari Hepsi icin yapilabilir tree cv model ", "id": "ozericyer/heart-desease-ml-classificationexercises", "size": "12098", "language": "python", "html_url": "https://www.kaggle.com/code/ozericyer/heart-desease-ml-classificationexercises", "git_url": "https://www.kaggle.com/code/ozericyer/heart-desease-ml-classificationexercises"}