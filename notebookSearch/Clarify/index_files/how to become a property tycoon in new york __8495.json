{"name": "how to become a property tycoon in new york ", "full_name": " h1 How to predict House Prices and hopefully become a Property Tycoon in New York h1 1 Load Data and Clean it h1 2 Data Inspection h2 2 1 Dependent Variable Inspection h2 2 2 Independent Variables Inspection h1 3 Modelling h2 3 1 Data Preparation h2 3 2 Split into Training Testing Data h2 3 3 Running the Different Models h1 4 Conclusion and Next Steps ", "stargazers_count": 0, "forks_count": 0, "description": " How to predict House Prices and hopefully become a Property Tycoon in New York I am going to clean and visualise data and build a model to predict housing prices in New York Everything is done obviously with the aim of becoming a property tycoon one of the most important things to know in order to achieve that is the value of a property So without further ado let s go for it 1 Load Data and Clean it Let s update the BOROUGH names first according to the instructions found on Kaggle What we can see by looking at the first few rows is that the column Unnamed 0 is an artifact from the data load and is not needed The column EASEMENT is completely empty and will be deleted And there some missing Sale Prices Update Data I have already done some prior inspection of the data and am now updating the dataset by deleting columns and changing the data type of some of the variables After updating the data let s check if there are any duplicate values in here There are 765 duplicates Let s remove them High Level Data Inspection and Validation Let s show this visually It s easier that way to see where the NULL values are and how many there are That s a shame 20 of all Sale Prices are NULL which is what I wanted to predict I could still predict a price for those cases but there is no way of verifying the accuracy of the predictions Those observations will have to be deleted There are also around a third of all observations with missing Square Feet data There is potential to impute those values but we will have to see how well that will work After removing the missing SALE PRICES we are left with 70k observations down from 85k at the very start Now let s get an overview of some descriptive stats of the numerical variables in the data set Some interesting observations to note 1 There are ZIP CODES with a value of 0 which is probably wrong 2 75 of properties have no COMMERCIAL UNITS 3 At least 50 of all properties have only 1 TOTAL UNIT I am not quite sure what to make of that yet 4 There are properties have 0 SQUARE FEET which shouldn t be possible unless they don t exist yet or the data is wrong 5 Some buildings were built in the YEAR 0 which again is wrong 6 Some properties have a SALE PRICE of 0 which is also wrong or a transfer but not actually a sale 2 Data Inspection Again I have already done some data inspection and I will not show those variables that aren t useful for the model I will first look at the dependent variable SALE PRICE which is the one I want to predict After that I will look at the independent variables which are the ones I use to predict the price 2 1 Dependent Variable Inspection SALE PRICE Let s start with the dependent variable as this is the one I want to predict First some plots What we see from the two graphs above is that there are a lot of outliers Maybe this isn t all that surprising given that Manhattan is home to a lot of very expensive property From the descriptive statistics we could also tell that 75 of the properties in this dataset are cheaper than 950 000 USD There are also a fair number of properties cheaper than 100 000 USD which seems too cheap in my opinion I have done a few iterations of what a good cap on sales prices is and settled for 1 property needs to be more expensive than 100 000 USD 2 property needs to be cheaper than 5 000 000 USD Everything is else is a different animal and mixing all together in one model will decrease accuracy And we re down from 70k observations to 55k which is 79 remaining That s a fair chunk of data I ve been toying with the idea of clustering properties on their SALE PRICE classifiying them as something like cheap normal expensive and luxury in order to avoid this issue I will work on this in a future iteration Anyways let s check out the same graphs again Recap and Revisit of Entire Data Set After I ve removed some observations due to their prices which I have treated as outliers how many NULL SQUARE FEET observations remain Unfortunately there are still a third of observations remaining that contain no SQAURE FEET data It turns out SPOILER ALERT that this is the best predictor of SALE PRICE in this dataset which means I want to keep as much data as possible instead of just throwing them away I ll show the importance of SQAURE FEET in the result section at the end of the workbook 2 2 Independent Variables Inspection I have already looked at each variable in more detail and only show the ones I am going to keep for the model Currently I have removed all the NULL and outlier data of SQUARE FEET and TOTAL UNITS but in a future iteration I will try to impute some data points to keep as much as data as possible SQUARE FEET I need to get rid of the NULL values and a few outliers Total Units I am deleting the outliers with very large numbers of TOTAL UNITS and those with 0 units Those are only a handful of observations though In addition I get rid of the observations the sum of COMMERCIAL UNITS and RESIDENTIAL UNITS doesn t match TOTAL UNITS YEAR BUILT Next one up is YEAR BUILT There seem to be some buildings that were built in the year 0 which can t be correct Let s remove those What does the plot look like now OK that s a lot more realistic However YEAR BUILT isn t quite the variable we are looking for What is more interesting is the BUILDING AGE Both contain similar information the second is however a bit more practical So let s create it BOROUGH This is all in good shape and no surpises here Building Class Category Some of the categories could potentially be merged in a future iteration More Data Visualisations 3 Modelling 3 1 Data Preparation scikit works best with normalized data i e data that has a mean around 0 and a distribution around that In the next section I will normalise standardise the data and also take the log in order to get rid of the skewness and to allow for a more normal distribution But first I get all the relevant variables and one hot encode the categrical variables which is necessary for scikit to work The numerical variables don t need one hot encoding but will have to be normalised Transforming the dependent variable SALE PRICE This is what SALE PRICE looks before the transformation and this what it looks after Transforming the independent variables Some of the variables contain zeroes which is why I need to add 1 so that I can take the log before normalising it you can see that in the table below Using the log allows me to get rid of the skew in the data and have a more normal distribution The reason why I need to add 1 is because I can t take the log of 0 it is not defined The log of 1 however is UNITS SQUARE FEET BUILDING AGE 3 2 Split into Training Testing Data The step is necessary to ensure that the model is flexible and general enough so that can predict accurately unseen or new data I train the model with the training data and then check how good it performs on the unseen testing data 3 3 Running the Different Models Finally the moment we have all been waiting for I will now test a few different models to see which one performs best In a next iteration I will also fine tune these to get an even better result I will use 1 Linear Regression 2 Random Forest Regression 3 Ridge Regression 4 and ElasticNet 1 Linear Regression 2 Random Forest Feature Importance of Random Forest 3 Ridge Regression 4 ElasticNet and GridSearch 4 Conclusion and Next Steps 1 An untuned Random Forest Regression managed to get a R2 of 0 40 which is certainly not great but maybe not bad given the limited amount of data available SQUARE FEET BUILDING AGE and BOROUGH were the most important features determining the SALE PRICE 2 We only used 30k observations out of a potential of 70k Remember that there were a fair amount of NULL SALE PRICE observations and duplicates etc So the original 85k don t count In a next iteration I will therefore explore the possibilities to impute the SQUARE FEET data because I deleted a lot here The same is true for UNITS data 3 Instead of throwing outlier SALE PRICE data I will try to use clustering to classify properties w hich would allow me to keep the outlier data maybe  Import the modules Data Scaler Regression Metrics Read the data Renaming BOROUGHS Change the settings so that you can see all columns of the dataframe when calling df head EASE MEANT is empty and can be dropped Unnamed 0 is an artifact from the data load and can be deleted SALE PRICE is object but should be numeric LAND and GROSS SQUARE FEET is object but should be numeric SALE DATE is object but should be datetime Both TAX CLASS attributes should be categorical Delete the duplicates and check that it worked Check the number of rows and columns Get a high level overview of the data types the amount of NULL values etc Capture the necessary data Plot number of available data per variable Remove observations with missing SALE PRICE Set the size of the plot Plot the data and configure the settings Set the size of the plot Plot the data and configure the settings Set the size of the plot Get the data and format it Plot the data and configure the settings Remove observations that fall outside those caps Set the size of the plot Plot the data and configure the settings Set the size of the plot Plot the data and configure the settings Set the size of the plot Get the data and format it Plot the data and configure the settings Capture the necessary data Plot number of available data per variable Removes all NULL values Keeps properties with fewer than 20 000 Square Feet which is about 2 000 Square Metres Only a handful of properties with 0 total units are remaining and they will now be deleted Remove data where commercial residential doesn t equal total units Correlation Matrix Compute the correlation matrix Generate a mask for the upper triangle Set up the matplotlib figure Generate a custom diverging colormap Draw the heatmap with the mask and correct aspect ratio Choose only the variables I want to use in the model Select the variables to be one hot encoded For each categorical column find the unique number of categories This tells us how many columns we are adding to the dataset Convert categorical variables into dummy indicator variables i e one hot encoding Delete the old columns and add the new one hot encoded variables Take the log and normalise Add 1 to Units Take the log and standardise Add 1 to Units Take the log and standardise Add 1 to BUILDING AGE Take the log and standardise Split data into training and testing set with 80 of the data going into training X are the variables features that help predict y which tells us whether an employee left or stayed This is done for both Create the regressor linreg Fit the regressor to the training data Predict the labels of the test set y pred Compute 5 fold cross validation scores cv scores Print the 5 fold cross validation scores Compute 5 fold cross validation scores cv scores Print the 5 fold cross validation scores Plot the feature importances of the forest Import necessary modules Setup the array of alphas and lists to store scores Create a ridge regressor ridge Compute scores over range of alphas Specify the alpha value to use ridge alpha Perform 10 fold CV ridge cv scores Append the mean of ridge cv scores to ridge scores Append the std of ridge cv scores to ridge scores std Display the plot Instantiate a ridge regressor ridge Fit the model Perform 5 fold cross validation ridge cv Print the 5 fold cross validation scores Create the hyperparameter grid Instantiate the ElasticNet regressor elastic net Setup the GridSearchCV object gm cv Fit it to the training data Predict on the test set and compute metrics ", "id": "akosciansky/how-to-become-a-property-tycoon-in-new-york", "size": "8495", "language": "python", "html_url": "https://www.kaggle.com/code/akosciansky/how-to-become-a-property-tycoon-in-new-york", "git_url": "https://www.kaggle.com/code/akosciansky/how-to-become-a-property-tycoon-in-new-york"}