{"name": "kernelbecbda8662 ", "full_name": " h2 Introduction h1 1 Get your data h1 2 Explore the dataset h1 3 Define your objective and metrics h1 4 Set a baseline model h3 4 1 Create features h3 4 2 Run a simple algorithm h3 4 3 Optimize the hyper parameters of your algorithm h2 5 Improve features and model h3 5 1 Better features h3 5 2 Better Algorithms h2 6 Use Deep Learning ", "stargazers_count": 0, "forks_count": 0, "description": " center Introduction center In this tutorial we work on breast histology images Our goal is to classify Invasive Ductal Carcinoma IDC images vs non IDC images using a standard data science pipeline Explore your dataset Define your objective and a metric Set a baseline algorithm features model parameter optimization Improve it The dataset has been curated from Andrew Janowczyk website http www andrewjanowczyk com use case 6 invasive ductal carcinoma idc segmentation center 1 Get your data center The first step is simply to get your data Data can have any modality csv file dicom image text file mp3 record Then using your favorite programming language mainly python or R and appopriated packages pandas numpy you can read your data center 2 Explore the dataset center Let s explore your dataset now It is fundamental to become familiar with your data especially if you re not an expert This step includes Checking data issues of any sort missing values outliers unreadable files etc Searching bias in the dataset For instance imagine you have to predict the age of a patient based on is height weight and other values and you didn t see that birth of date was in these values your algorithm is going to learn a completly dumb thing Plot histograms correlations use standard data analysis techniques sur as PCA Try to evaluate the difficulty of the task you want to perform And of course have a look to related works on this dataset Maybe someone did the work for you Here we can observe that colors are quite different between positive images and negative images Colors could be an easy way to classify these images center 3 Define your objective and metrics center Now that you are familiar with your data you have to define your objective and the way you will measure how you achieved it The machine learning scheme is quite different from the statistics scheme we evaluate a model based on its predictive performance and not on the probability to observe the data given the model In our case the objective or task is binary classification of images and our measure performance will be the accuracy percentage of good classification Note that accuracy is not always a good measure if you have an unbalanced dataset with only 1 of positive examples predicting always 0 negative will lead to a 99 accuracy The measure is often called metric or loss It depends on what you want interested by the trade off precision recall use F1 score Interested by the trade off true positive detection false positive detection Use ROC AUC To measure the predictive performance using our metric we will use cross validation The idea is to split the dataset into 2 groups the training set that we will use to calibrate the parameters of a model and the test set that we will use to evaluate the performance of the model using our metric In cross validation we split the dataset multiple times ex 80 20 or 90 10 and average the performances on the diffrent test sets A common technique is to partition the dataset into N equal parts called folds and to split N times the dataset such that each part will be once the test set For instance if you divide your dataset into 5 folds 1 2 3 4 5 then you will split 5 times the dataset train 2 3 4 5 test 1 evaluate performance on the test set to get score1 train 1 3 4 5 test 2 score2 train 1 2 4 5 test 3 score3 train 1 2 3 5 test 4 score4 train 1 2 3 4 test 5 score5 predictive performance score1 score2 score3 score4 score5 5 center 4 Set a baseline model center Now it s time to create our first model A good practice is to set a baseline i e a very simple model It permits to quickly have a first score to compare with when using more complex models and it also often helps a lot to organize your code and data A common machine learning way to create a new model is 1 Extract features from your data In our case we saw that colors of the images could be a good feature Here expert knowledge is welcome 2 Use a learning algorithm you think appropriate k nearest neighbors linear or logistic regression random forests SVMs neural networks and XGboost 3 Optimize the hyper parameters of your learning algorithm center 4 1 Create features center The PCA shows that our simple histogram feature already permits to linearly separate a bit positive and negative images Good point center 4 2 Run a simple algorithm center center 4 3 Optimize the hyper parameters of your algorithm center center 5 Improve features and model center center 5 1 Better features center center 5 2 Better Algorithms center center 6 Use Deep Learning center Deep learning for images is end to end there is no need to exctract features from the image because the algorithm learns itself these features from raw data Neural networks are not easy to train especially from scratch and require a lot of data But when possible it shows amazing performance In the code below we run quite a randomly designed neural network to see how it works Don t expect it to have very good results Also use it as a black box as this tuto doesn t aim to explain how deep learning works Training a neural network can be long so we just use 1 fold A neural network is trained by showing him the dataset multiple times Each time the network saw the entire dataset is called epoch On the learning curve you can observe the accuracy of the network on the training set in red and on the test set in blue The more epochs the more the networks learn to recognize the training imagss and generalize it s performance on the test set But if we go too far the network memorized perfectly the training set very high accuracy on training which leads to poor generalization low accuracy on the test this phenomenom is called overfitting  I did the work for you images labels associated to images 0 no cancer 1 cancer How many images What is the size of the images Looking at some images images with label 0 no cancer negative image images with label 1 cancer positive image Each batch plot 50 negative images on the left and 50 positive images on the right Prepare the folds for a cross validation We use the very useful scikit learn package Here we use stratified cross validation the proportion of positive and negative examples is the same in all folds Let s have a look to the split size and We look at RGB histograms represent colors Histogram counts the number of pixels with a certain intensity between 0 and 255 for each color red green and blue A peak at 255 for all colors mean a lot of white Try 0 1 2 for negative images and 1 2 3 for positive images and compare the histograms Our feature will be the concatenation of the 3 histograms red green and blue We decide to keep 50 bins in the histogram but you can try other values up to 255 Here we visualize our features in 2D using PCA colored by the positive red negative blue class Try to replace PCA by SpectralEmbedding TSNE Here we will use the k nearest neighbors algorithm given a new image find the k e g k 10 images in the training set with the most similar histograms If m of these neighors e g m 6 have label 1 then output p 60 This function run train the model For each fold Train the model on the training data And predict the results on the test data Return all the predictions probabilities between 0 and 1 Here we use kNN model And call the function run model Let s print the results How does the accuracy evolves with the number of neighbors Here we use features extracted from a deep neural network we achieve much better results with the same algorithm that our simple histogram features Nearest Neighbors The code to extract deep learning features a bit long to run model ResNet50 weights imagenet include top False pooling max Logisitic Regression Random Forests We use keras library wuth tensorflow backend Special callback to see learning curves Preprocess the data center and normalize Create the model first layer convolution second layer pooling reduce the size of the image per 3 output 1 value between 0 and 1 probability to have cancer Use binary crossentropy as a loss function try to add loss to see the loss learning curve ", "id": "alaedine/kernelbecbda8662", "size": "6152", "language": "python", "html_url": "https://www.kaggle.com/code/alaedine/kernelbecbda8662", "git_url": "https://www.kaggle.com/code/alaedine/kernelbecbda8662"}