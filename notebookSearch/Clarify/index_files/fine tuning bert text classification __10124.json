{"name": "fine tuning bert text classification ", "full_name": " h1 Fine Tune BERT for Text Classification with TensorFlow h2 Prerequisites h2 Initial Set Up h3 Install TensorFlow and TensorFlow Model Garden h2 Some Initial Imports and Checks h4 Logging into wandb h2 Lets Get the Dataset h3 Get to Know your data Some Basic EDA h1 Taming the Data h2 Lets BERT Get the Pre trained BERT Model from TensorFlow Hub h3 Checking out some of the training samples and their tokenized ids h2 Lets Get That Data Ready Tokenize and Preprocess Text for BERT h3 Wrapping the Python Function into a TensorFlow op for Eager Execution h2 Let the Data Flow Creating the final input pipeline using tf data h1 Lets Model Our Way to Glory h2 Create The Model h2 Let Us Train h4 W B Experiment Tracking h3 Lets Evaluate h3 Lets Look at some Graphs h3 Saving the models and model Versioning h4 W B Artifacts h3 Quick Sneak Peek into the W B Dashboard h3 References ", "stargazers_count": 0, "forks_count": 0, "description": " h1 Fine Tune BERT for Text Classification with TensorFlow h1 div align center img width 512px src https drive google com uc id 1mBqrfxng42SgSXvK62V1C67Or vgrsVm p style text align center color gray Figure 1 BERT Classification Model p div We will be using GPU accelerated Kernel for this tutorial as we would require a GPU to fine tune BERT Prerequisites Willingness to learn Growth Mindset is all you need Some basic idea about Tensorflow Keras Some Python to follow along with the code Initial Set Up Install TensorFlow and TensorFlow Model Garden Cloning the Github Repo for tensorflow models depth 1 during cloning Git will only get the latest copy of the relevant files It can save you a lot of space and time b lets us clone a specific branch only Please match it with your tensorflow 2 x version Note After installing the required Python packages you ll need to restart the Colab Runtime Engine Run Restart and clear all cell outputs NOTE ANYTHING BEFORE THIS CELL SHOULD ONLY BE RUN ONCE ONLY DURING THE INITIAL SETUP Some Initial Imports and Checks A Healthy practice for any ML practioner is to do a clean experiment tracking such that reasults are reproducable and trackable For this kernel we will be looking into Weights and Biases https wandb ai site for experiment tracking Here are Four main things that W B offers Experiment Tracking Tracking ML Experiments and logging various parameters and metrics on a clean dashboard Sweeps Hyper parameter tuning You can run multiple experiments with different hyper parameters and track them Artifacts Storing Datasets models and other files for version tracking Reports we can create reports on experiments and project levels We will be looking into Experiment Trancking and Artifacts Logging into wandb First things first we need to create a free account on W B https wandb ai site Then let us access our authorization API key https wandb ai authorize and add it to kaggle s secret key for hassle free authentication More details about Kaggle s Secret key feature on https www kaggle com product feedback 114053 Lets Get the Dataset The data we will use is the dataset provided on the Quora Insincere Questions Classification competition on Kaggle https www kaggle com c quora insincere questions classification data Please feel free to download the train set from kaggle or use the link below to download the train csv from that competition https archive org download quora dataset train csv quora dataset train csv zip https archive org download quora dataset train csv quora dataset train csv zip Let us decompress and read the data into a pandas DataFrame Get to Know your data Some Basic EDA So it looks like the train and validation set are similar in terms of class imbalance and the various lengths in the question texts Even the distribution of question length in words and characters is very similar It looks like a good train test split so far Taming the Data Lets BERT Get the Pre trained BERT Model from TensorFlow Hub We will be using the uncased BERT present in the tfhub In order to prepare the text to be given to the BERT layer we need to first tokenize our words The tokenizer here is present as a model asset and will do uncasing for us as well Setting all parameters in form of a dictionary so any changes if needed can be made here Checking out some of the training samples and their tokenized ids Lets Get That Data Ready Tokenize and Preprocess Text for BERT Each line of the dataset is composed of the review text and its label Data preprocessing consists of transforming text to BERT input features input word ids input mask segment ids input type ids Input Word Ids Output of our tokenizer converting each sentence into a set of token ids Input Masks Since we are padding all the sequences to 128 max sequence length it is important that we create some sort of mask to make sure those paddings do not interfere with the actual text tokens Therefore we need a generate input mask blocking the paddings The mask has 1 for real tokens and 0 for padding tokens Only real tokens are attended to Segment Ids For out task of text classification since there is only one sequence the segment ids input type ids is essentially just a vector of 0s Bert was trained on two tasks fill in randomly masked words from a sentence given two sentences which sentence came first You want to use Dataset map https www tensorflow org api docs python tf data Dataset map to apply this function to each element of the dataset Dataset map https www tensorflow org api docs python tf data Dataset map runs in graph mode Graph tensors do not have a value In graph mode you can only use TensorFlow Ops and functions So you can t map this function directly You need to wrap it in a tf py function https www tensorflow org api docs python tf py function The tf py function https www tensorflow org api docs python tf py function will pass regular tensors with a value and a numpy method to access it to the wrapped python function Wrapping the Python Function into a TensorFlow op for Eager Execution Let the Data Flow Creating the final input pipeline using tf data The resulting tf data Datasets return features labels pairs as expected by keras Model fit https www tensorflow org api docs python tf keras Model fit Lets Model Our Way to Glory Create The Model There are two outputs from the BERT Layer A pooled output of shape batch size 768 with representations for the entire input sequences A sequence output of shape batch size max seq length 768 with representations for each input token in context For the classification task we are only concerned with the pooled output Let Us Train One drawback of the tf hub is that we import the entire module as a layer in keras as a result of which we dont see the parameters and layers in the model summary The official tfhub page states that All parameters in the module are trainable and fine tuning all parameters is the recommended practice Therefore we will go ahead and train teh entire model without freezing anything W B Experiment Tracking In order to start the expirment tracking we will be creating runs on W B wandb init It initializes the run with basic project information parameters project The project name this will create a new project tab where all the experiments for this project will be tracked config A dictionary of all parameters and hyper parameters we wish to track group optional but would help us to group by different parameters later on job type to describe the job type it would help in grouping different experiments later eg train evaluate etc In order to Log all the different metrics we will use a simple callback provided by W B WandCallback https docs wandb ai guides integrations keras Yes Its as simple as adding a callback D Lets Evaluate Let us do an evaluation on the validation set and log the scores using weights and biases wandb log Log a dict of scalars metrics like accuracy and loss and any other type of wandb object Here we will pass the evaluation dictionary as it is and log it Lets Look at some Graphs These Graphs will mainly be useful when we are training for more epochs and more data All these graphs are actually directly logged on the wandb dashboard https wandb ai akshayuppal12 Finetune BERT Text Classification runs 29thnm00 workspace user akshayuppal12 created for this run To still give out a method to generate graphs explicitly here is some very basic code Since we just trained for a very limited data and less epoch these graphs generated here are not as represenetative so leaving this here more interms of a place holder for the reader to experiment with Saving the models and model Versioning W B Artifacts For saving the models and making it easier to track different experiments I will be using wandb artifacts W B Artifacts are a way to save your datasets and models Within a run there are three steps for creating and saving a model Artifact Create an empty Artifact with wandb Artifact Add your model file to the Artifact with wandb add file Call wandb log artifact to save the Artifact Artifact dashboard https wandb ai akshayuppal12 Finetune BERT Text Classification artifacts model BERT EN UNCASED 48ffa3e14aba242a5113 Model versioning and more Quick Sneak Peek into the W B Dashboard Things to note Grouping of experiments and runs Visualizations of all training logs and metrics Visualizations for system metrics could be useful when training on cloud instances or physical GPU machines Hyperparmeter tracking in the tabular form Artifacts Model versioning and storage img src https i imgur com CD7iPK1 gif Hopefully this was useful for you and by now you have a small kickstart on training and utilizing BERT for a variety of downstream tasks like classification Named Entity Recognition Sentence filling and many more You can check out and get the entire code as a notebook and run it on colab from this Github Repo https github com au1206 Fine Tuning BERT If this was helpful consider sharing it with more people so they can also learn about it Coming up Next BERT Annotated Paper Write up on Transformers and its workings For some annotated reader friendly research papers on advanced concepts and tutorials like these please visit https au1206 github io If you made it this far please consider leaving feedback so I can improve and also if you liked it consider upvoting UPDATE BERT Annotated Paper and Beyond https au1206 github io annotated 20paper BERT References W B usage and intro https www kaggle com ayuraj experiment tracking with weights and biases  install requirements to use tensorflow models repository you may have to restart the runtime afterwards also ignore any ERRORS popping up at this step TO LOAD DATA FROM ARCHIVE LINK import numpy as np import pandas as pd from sklearn model selection import train test split df pd read csv https archive org download quora dataset train csv quora dataset train csv zip compression zip low memory False print df shape TO LOAD DATA FROM KAGGLE label 0 non toxic label 1 toxic Since the dataset is very imbalanced we will keep the same distribution in both train and test set by stratifying it based on the labels using small portions of the data as the over all dataset would take ages to train feel free to include more data by changing train size TRAIN SET VALIDATION SET TRAIN SET VALIDATION SET we want the dataset to be created and processed on the cpu lets look at 3 samples from train set Setting some parameters Label categories maximum length of token input sequences Get BERT layer and tokenizer All details here https tfhub dev tensorflow bert en uncased L 12 H 768 A 12 2 checks if the bert layer we are using is uncased or not This provides a function to convert row to input features and label this uses the classifier data lib which is a class defined in the tensorflow model garden we installed earlier since we only have 1 sentence for classification purpose textr b is None since only 1 example the index 0 py func doesn t set the shape of the returned tensors the final datapoint passed to the model is of the format a dictionary as x and labels the dictionary have keys which should obv match Now we will simply apply the transformation to our train and test datasets train valid train data spec we can finally see the input datapoint is now converted to the BERT specific input tensor valid data spec Building the model input BERT Layer Classification Head for classification we only care about the pooled output at this point we can play around with the classification head based on the downstream tasks and its complexity inputs coming from the function Calling the create model function to get the keras based functional model using adam with a lr of 2 10 5 loss as binary cross entropy as only 2 classes and similarly binary accuracy Update CONFIG dict with the name of the model Initialize W B run Train model setting low epochs as It starts to overfit with this limited data please feel free to change Initialize a new run for the evaluation job Model Evaluation on validation set Log scores using wandb log Finish the run Save model Initialize a new W B run for saving the model changing the job type Save model as Model Artifact Finish W B run ", "id": "au1206/fine-tuning-bert-text-classification", "size": "10124", "language": "python", "html_url": "https://www.kaggle.com/code/au1206/fine-tuning-bert-text-classification", "git_url": "https://www.kaggle.com/code/au1206/fine-tuning-bert-text-classification"}