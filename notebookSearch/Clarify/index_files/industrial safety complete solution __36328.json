{"name": "industrial safety complete solution ", "full_name": " h1 AIML Capstone Project Industrial Safety and Health Analytics Database h2 Table of Contents h2 1 Overview h3 Domain Industrial safety NLP based Chatbot h3 Context h3 Data Description h2 2 Import the necessary libraries h4 Setting Options h2 3 Data Collection h4 Shape of the data h4 Data type of each attribute h2 Data Collection Summary h2 4 Data Cleansing h4 Remove Unnamed 0 and Rename Data Countries Genre Employee or Third Party columns h4 Check Duplicates h4 Drop Duplicates h4 Check Outliers h4 Check Missing Values h2 Data Cleansing Summary h2 5 Data Pre processing h2 6 EDA Data Analysis and Preparation h4 Variable Identification h4 Univariate Analysis h4 Bivariate Analysis and Hypothesis testing h4 Study Summary Statistics h4 Study Correlation h2 EDA Summary h2 7 NLP Analysis h2 8 NLP Pre processing h4 Get the Length of each line and find the maximum length h4 WordCloud h4 NLP text summary statistics h2 NLP Pre processing Summary h2 9 Feature Engineering h4 Variable Creation Word2Vec Embeddings h4 Variable Creation Glove Word Embeddings h4 Variable Creation TFIDF Features h4 Variable Creation Label Encoding h4 Combine Glove and Encoded Features h4 Combine TFIDF and Encoded Features h4 Sampling Techniques Create Training and Test Set h4 Resampling Techniques Oversample minority class h4 SMOTE Generate synthetic samples upsample smaller class h4 Varible Tansformation Normalization and Scaling h4 Use PCA Extract Principal Components that capture about 95 of the variance in the data h2 10 Design train and test machine learning classifiers h4 Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be h4 Define MultiClass Logloss h4 Train and test model h4 Train and test all models h4 Model with Hyperparameter Tuning h4 1 Modelling Logistic Regression h4 2 Decision Tree Random Forest Classifier h4 3 Modelling Logistic Regression Oversampling h4 4 Modelling Logistic Regression SMOTE h4 All models Original data h4 All models Oversampling data h4 All models SMOTE data h4 Hyperparameter tuning with original features h4 Bootstrap Sampling RandomForestClassifier h4 Bootstrap Sampling AdaBoostClassifier h4 Bootstrap Sampling XGBClassifier h2 11 Design train and test Neural networks classifiers h4 Get ANN Multiclass Classification Metrics h4 Convert Classification to Numeric problem h4 Multiclass classification Target variable One hot encoded h4 Multiclass classification Target variable One hot encoded with SMOTE data h2 12 Design train and test RNN or LSTM classifiers h4 Architecture h2 13 Conclusion h2 14 Recommendations h2 15 Limitations h2 16 Closing Reflections h2 17 References ", "stargazers_count": 0, "forks_count": 0, "description": " AIML Capstone Project Industrial Safety and Health Analytics Database a id table of contents a Table of Contents 1 Overview overview 2 Import the necessary libraries import libraries 3 Data Collection data collection 4 Data Cleansing data cleansing 5 Data Pre processing data preprocessing 6 EDA Data Analysis and Preparation eda Univariate Analysis univariate analysis Bivariate Analysis and Hypothesis testing bivariate analysis 7 NLP Analysis nlp analysis 8 NLP Pre processing nlp pre processing Word Cloud wordcloud 9 Feature Engineering feature engineering 10 Design train and test machine learning classifiers ml models All models Original data ml models original data All models Oversampling data ml models oversampling data All models SMOTE data ml models smote data Hyperparameter tuning with original features ml models hyperparam tuning Bootstrap Sampling ml models bootstrap sampling 11 Design train and test Neural networks classifiers ann models Convert Classification to Numeric problem ann models clas to num problem Multiclass classification Target variable One hot encoded ann models multi class 12 Design train and test RNN or LSTM classifiers nlp models Creating a Model with Text Inputs Only nlp models text input Creating a Model with Categorical features Only nlp models cat features Creating a Model with Multiple Inputs nlp models multiple input 13 Conclusion conclusion 14 Recommendations recommendations 15 Limitations limitations 16 Closing Reflections closing reflections 17 References references a id overview a 1 Overview Domain Industrial safety NLP based Chatbot Context The database comes from one of the biggest industry in Brazil and in the world It is an urgent need for industries companies around the globe to understand why employees still suffer some injuries accidents in plants Sometimes they also die in such environment Data Description This The database is basically records of accidents from 12 different plants in 03 different countries which every line in the data is an occurrence of an accident Columns description Data timestamp or time date information Countries which country the accident occurred anonymised Local the city where the manufacturing plant is located anonymised Industry sector which sector the plant belongs to Accident level from I to VI it registers how severe was the accident I means not severe but VI means very severe Potential Accident Level Depending on the Accident Level the database also registers how severe the accident could have been due to other factors involved in the accident Genre if the person is male of female Employee or Third Party if the injured person is an employee or a third party Critical Risk some description of the risk involved in the accident Description Detailed description of how the accident happened Link to download the dataset https www kaggle com ihmstefanini industrial safety and health analytics database Table of Contents table of contents a id import libraries a 2 Import the necessary libraries Firstly let s select TensorFlow version 2 x in colab Setting Options Table of Contents table of contents a id data collection a 3 Data Collection Shape of the data Data type of each attribute From the above output we see that except first column all other columns datatype is object Categorical columns Countries Local Industry Sector Accident Level Potential Accident Level Genre Employee or Third Party Critical Risk Description Date column Data Data Collection Summary 1 There are about 425 rows and 11 columns in the dataset 2 We noticed that except a date column all other columns are categorical columns Table of Contents table of contents a id data cleansing a 4 Data Cleansing Remove Unnamed 0 and Rename Data Countries Genre Employee or Third Party columns Check Duplicates There is no need to worry about preserving the data it is already a part of the industry dataset and we can merely remove or drop these rows from your cleaned data Drop Duplicates Check Outliers As we know there is no concept of outliers detection in categorical variables nominal and ordinal as each value is count as labels Let s check the unique and frequency mode of each variable We observed that there are records of accidents from 1st Jan 2016 to 9th July 2017 in every month So there are no outliers in the Date column There are only three country types so there are no outliers in Country column There are 12 Local cities where manufacturing plant is located and it s types are in sequence so there are no outliers in Local column There are only three Industry Sector types which are in sequence so there are no outliers in Industry Sector column There are only five Accident Level types which are in sequence so there are no outliers in Accident Level column There are only six Potential Accident Level types which are in sequence so there are no outliers in Potential Accident Level column There are only two Gender types in the provided data so there are no outliers in Gender column There are only three Employee types in the provided data so there are no outliers in Gender column There are quite a lot of Critical risk descriptions and we don t see any outliers but with the help of SME we can decide whether this column has outliers or not Check Missing Values Data Cleansing Summary 1 Removed Unnamed 0 column and renamed Data Countries Genre Employee or Third Party columns in the dataset 2 We had 7 duplicate instances in the dataset and dropped those duplicates 3 There are no outliers in the dataset 4 No missing values in dataset 5 We are left with 418 rows and 10 columns after data cleansing Table of Contents table of contents a id data pre processing a 5 Data Pre processing To better understand the data I am extracting the day month and year from Date column and creating new features such as weekday weekofyear As we know this database comes from one of the biggest industry in Brazil which has four climatological seasos as below https seasonsyear com Brazil Spring September to November Summer December to February Autumn March to May Winter June to August We can create seasonal variable based on month variable We can create holidays variable based on Brazil holidays list from 2016 and 2017 Another national holidays are election days There are a plenty of unofficial ethnic and religious holidays in Brazil Octoberfest Brazilian Carnival Kinderfest Fenaostra Fenachopp Musikfest Schutzenfest Kegelfest Cavalhadas Oberlandfest Tirolerfest Marejada are among them Note Considering official holidays only Table of Contents table of contents a id eda a 6 EDA Data Analysis and Preparation Variable Identification Target variable Accident Level Potential Accident Level Predictors Input varibles Date Country Local Industry Sector Gender Employee type Critical Risk Description a id univariate analysis a Univariate Analysis Country 59 accidents occurred in Country 01 31 accidents occurred in Country 02 10 accidents occurred in Country 03 Local Highest manufacturing plants are located in Local 03 city Lowest manufacturing plants are located in Local 09 city Industry Sector 57 manufacturing plants belongs to Mining sector 32 manufacturing plants belongs to Metals sector 11 manufacturing plants belongs to Others sector Accident Levels The number of accidents decreases as the Accident Level increases The number of accidents increases as the Potential Accident Level increases Gender There are more men working in this industry as compared to women Employee type 44 Third party empoyees working in this industry 43 own empoyees working in this industry 13 Third party Remote empoyees working in this industry Critical Risk Because most part of the Critical Risks are classified as Others it is thought that there are too many risks to classify precisely And it is also thought that it takes so much time to analyze risks and reasons why the accidents occur Calendar Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month there are high number of accidents in 2016 and less in 2017 Number of accidents are high in beginning of the year and it keeps decreasing later Number of accidents are very high in particular days like 4 8 and 16 in every month Number of accidents increased during the middle of the week and declined since the middle of th week Table of Contents table of contents a id bivariate analysis a Bivariate Analysis and Hypothesis testing a Industry Sector by Countries Is the distribution of industry sector different significantly in differ countries or not Observations Metals and Mining industry sector plants are not available in Country 03 Distribution of industry sector differ significantly in each country But let s check the proportion of metals mining and others sector in Country 01 and is that difference is statistically significant 1 State the H0 and Ha Ho The proportions of industry sector is not differ in different countries Ha The proportions of industry sector is differ in different countries 2 Decide the significance level alpha 0 05 3 Identify the test statistic Z test of proportions 4 Calculate the p value using test statistic 5 Decide to Reject or Accept Null Hypothesis Hence we reject Null Hypothesis we have enough 95 evidence to prove that the mining sector in country 1 is differ from metals sector Hence we reject Null Hypothesis we have enough 95 evidence to prove that the mining sector in country 1 is differ from others sector b Employee type by Gender Is the distribution of employee type differ significantly in different genders Observations Proportion of third party employees in each gender is equal Proportion of third party remote employees in each gender is not equal Proportion of own employees in each gender is not equal But let s check is that difference is statistically significant 1 State the H0 and Ha Ho The proportions of own employees in each gender is equal Ha The proportions of own employees in each gender is not equal 2 Decide the significance level alpha 0 05 3 Identify the test statistic Z test of proportions 4 Calculate the p value using test statistic 5 Decide to Reject or Accept Null Hypothesis Hence we fail to reject Null Hypothesis we have enough 95 evidence to prove that the proportion of own employees in each gender is equal c Industry Sector by Gender Is the distribution of industry sector differ significantly in different genders Observations Proportion of Metals sector employees in each gender is not equal Proportion of Mining sector employees in each gender is not equal Proportion of Others sector employees in each gender is not equal d Accident Levels by Gender Is the distribution of accident levels and potential accident levels differ significantly in different genders Observations Proportion of accident levels in each gender is not equal and males have a higher accident levels than females There are many low risks at general accident level but many high risks at potential accident level e Accident Levels by Employee type Is the distribution of accident levels and potential accident levels differ significantly in different employee types Observations For both accident levels the incidence of Employee is higher at low accident levels but the incidence of Third parties seems to be slightly higher at high accident levels f Accident Levels by Month Is the distribution of accident levels and potential accident levels differ significantly in different months Observations Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year g Accident Levels by Weekday Is the distribution of accident levels and potential accident levels differ significantly in different weekday Observations Both of the two accident level is thought that non severe levels decreased in the first and the last of the week but severe levels did not changed much h Accident Levels by Seasons Is the distribution of accident levels and potential accident levels differ significantly in different seasons Observations Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year Study Summary Statistics Study Correlation Observations WeekofYear featuer is having very high positive correlation with Month feature EDA Summary Local Highest manufacturing plants are located in Local 03 city and lowest in Local 09 city Country Percentage of accidents occurred in respective countries 59 in Country 01 31 in Country 02 and 10 in Country 03 Industry Sector Percentage of manufacturing plants belongs to respective sectors 57 to Mining sector 32 to Metals sector and 11 to Others sector Country Industry Sector Metals and Mining industry sector plants are not available in Country 03 Distribution of industry sector differ significantly in each country Accident Levels The number of accidents decreases as the Accident Level increases and increases as the Potential Accident Level increases Gender There are more men working in this industry as compared to women Employee type 44 Third party empoyees 43 own empoyees and 13 Third party Remote empoyees working in this industry Gender Employee type Proportion of third party employees in each gender is equal third party remote employees in each gender is not equal and own employees in each gender is not equal Gender Industry Sector Proportion of Metals Mining and Others sector employees in each gender is not equal Gender Accident Levels Males have a higher accident levels than females There are many low risks at general accident level but many high risks at potential accident level Accident Levels Employee type For both accident levels the incidence of Employee is higher at low accident levels but the incidence of Third parties seems to be slightly higher at high accident levels Accident Levels Calendar Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month there are high number of accidents in 2016 and less in 2017 Number of accidents are high in beginning of the year and it keeps decreasing later Number of accidents are very high in particular days like 4 8 and 16 in every month Number of accidents increased during the middle of the week and declined since the middle of th week Both of the two accident level have the tendency that non severe levels decreased throughout the year but severe levels did not changed much and some of these levels increased slightly in the second half of the year Both of the two accident level is thought that non severe levels decreased in the first and the last of the week but severe levels did not changed much Critical Risk Most of the critical risks are classified as Others Table of Contents table of contents a id nlp analysis a 7 NLP Analysis Observations 74 of data where accident description 100 is captured in low accident level Based on some random headlines seen above it appears that the data is mostly lower cased Pre processing such as removing punctuations and lemmatization can be used There are few alphanumeric characters like 042 TC 06 Nv 3370 CX 212 captured in description where removing these characters might help There are digits in the description for e g level 326 Dumper 01 where removing the digits wouldn t help Observations 34 of data where accident description 100 is captured in high medium potential accident level 25 of data where accident description 100 is captured in medium potential accident level 23 of data where accident description 100 is captured in low potential accident level Based on some random headlines seen above it appears that the data is mostly lower cased Pre processing such as removing punctuations and lemmatization can be used There are few alphanumeric characters like AFO 755 captured in description where removing these characters might help There are digits in the description for e g ditch 3570 0 50 cm deep 30 kg where removing the digits wouldn t help Table of Contents table of contents a id nlp pre processing a 8 NLP Pre processing Few of the NLP pre processing steps taken before applying model on the data Converting to lower case avoid any capital cases Converting apostrophe to the standard lexicons Removing punctuations Lemmatization Removing stop words Get the Length of each line and find the maximum length As different lines are of different length We need to pad the our sequences using the max length a id wordcloud a WordCloud Observations There are many body related employee related movement related equipment related and accident related words Body related left right hand finger face foot and glove Employee related employee operator collaborator assistant worker and mechanic Movement related fall hit lift and slip Equipment related equipment pump meter drill truck and tube Accident related accident activity safety injury causing NLP text summary statistics NLP Pre processing Summary 74 of data where accident description 100 is captured in low accident level 34 of data where accident description 100 is captured in high medium potential accident level 25 of data where accident description 100 is captured in medium potential accident level 23 of data where accident description 100 is captured in low potential accident level Few of the NLP pre processing steps taken before applying model on the data Converting to lower case avoid any capital cases Converting apostrophe to the standard lexicons Removing punctuations Lemmatization Removing stop words After pre processing steps Minimum line length 64 Maximum line length 672 Minimum number of words 10 Maximum number of words 98 Table of Contents table of contents a id feature engineering a 9 Feature Engineering Variable Creation Word2Vec Embeddings Variable Creation Glove Word Embeddings Variable Creation TFIDF Features Variable Creation Label Encoding Combine Glove and Encoded Features Combine TFIDF and Encoded Features Sampling Techniques Create Training and Test Set Resampling Techniques Oversample minority class SMOTE Generate synthetic samples upsample smaller class Varible Tansformation Normalization and Scaling Use PCA Extract Principal Components that capture about 95 of the variance in the data Table of Contents table of contents a id ml models a 10 Design train and test machine learning classifiers Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be Define MultiClass Logloss Train and test model Train and test all models Model with Hyperparameter Tuning 1 Modelling Logistic Regression 2 Decision Tree Random Forest Classifier While in every machine learning problem it s a good rule of thumb to try a variety of algorithms it can be especially beneficial with imbalanced datasets Decision trees frequently perform well on imbalanced data They work by learning a hierarchy of if else questions and this can force both classes to be addressed 3 Modelling Logistic Regression Oversampling 4 Modelling Logistic Regression SMOTE Table of Contents table of contents a id ml models original data a All models Original data By comparing the results from all above methods we can select the best method as AdaBoost classifier with f1 score 65 38 a id ml models oversampling data a All models Oversampling data By comparing the results from all above methods we can select best method as Ridge classifier with f1 score 62 68 and all other methods are over fitting the training data a id ml models smote data a All models SMOTE data By comparing the results from all above methods all are over fitting the training data a id ml models hyperparam tuning a Hyperparameter tuning with original features 1 Logistic Regression Best F1 Score 0 629247 using C 0 1 penalty l1 solver liblinear 0 629247 0 026945 with C 0 1 penalty l1 solver liblinear 95 Confidence interval range 0 5754 0 6831 Total duration 428 40893173217773 2 Ridge Best F1 Score 0 616087 using alpha 0 02 0 616087 0 023211 with alpha 0 02 95 Confidence interval range 0 5697 0 6625 Total duration 2 5354909896850586 3 KNeighborsClassifier Best F1 Score 0 629247 using metric euclidean n neighbors 13 weights uniform 0 629247 0 026945 with metric euclidean n neighbors 13 weights uniform 95 Confidence interval range 0 5754 0 6831 Total duration 12 201840877532959 4 SVC Best F1 Score 0 629247 using C 1 0 gamma scale kernel rbf 0 629247 0 026945 with C 1 0 gamma scale kernel rbf 95 Confidence interval range 0 5754 0 6831 Total duration 7 190079927444458 5 RandomForestClassifier Best F1 Score 0 622235 using max features sqrt n estimators 1000 0 622235 0 028932 with max features sqrt n estimators 1000 95 Confidence interval range 0 5644 0 6801 Total duration 176 1380627155304 6 BaggingClassifier Best F1 Score 0 620604 using max samples 0 75 n estimators 10 0 620604 0 034210 with max samples 0 75 n estimators 10 95 Confidence interval range 0 5522 0 6890 Total duration 305 1015794277191 7 ExtraTreesClassifier Best F1 Score 0 628276 using max features log2 min samples split 13 n estimators 90 0 628276 0 028086 with max features log2 min samples split 13 n estimators 90 95 Confidence interval range 0 5721 0 6844 Total duration 688 0113415718079 8 AdaBoostClassifier Best F1 Score 0 630245 using learning rate 0 1 n estimators 70 0 630245 0 030167 with learning rate 0 1 n estimators 70 95 Confidence interval range 0 5699 0 6906 Total duration 51 544673681259155 9 GradientBoostingClassifier Best F1 Score 0 613227 using learning rate 0 1 n estimators 30 0 613227 0 033099 with learning rate 0 1 n estimators 30 95 Confidence interval range 0 5470 0 6794 Total duration 165 68569326400757 10 CatBoostClassifier Best F1 Score 0 629247 using depth 4 iterations 300 l2 leaf reg 4 learning rate 0 03 0 629247 0 026945 with depth 4 iterations 300 l2 leaf reg 4 learning rate 0 03 95 Confidence interval range 0 5754 0 6831 Total duration 6367 161168336868 11 LGBMClassifier Best F1 Score 0 629247 using bagging fraction 0 5 bagging frequency 8 boosting type dart early stopping rounds 200 feature fraction 0 8 learning rate 0 0001 max depth 10 metric multi logloss min child samples 486 min child weight 0 01 min data in leaf 90 n estimators 1000 num class 5 num leaves 1550 objective multiclass verbosity 1 0 629247 0 026945 with bagging fraction 0 5 bagging frequency 8 boosting type dart early stopping rounds 200 feature fraction 0 8 learning rate 0 0001 max depth 10 metric multi logloss min child samples 486 min child weight 0 01 min data in leaf 90 n estimators 1000 num class 5 num leaves 1550 objective multiclass verbosity 1 95 Confidence interval range 0 5754 0 6831 Total duration 2430 5182700157166 Table of Contents table of contents a id ml models bootstrap sampling a Bootstrap Sampling RandomForestClassifier Bootstrap Sampling AdaBoostClassifier Bootstrap Sampling XGBClassifier Table of Contents table of contents a id ann models a 11 Design train and test Neural networks classifiers Get ANN Multiclass Classification Metrics a id ann models clas to num problem a Convert Classification to Numeric problem In this section we will create a classification model that uses categorical columns and tf idf features from accident description and label encoded target variable We can use simple densely connected neural networks to make predictions Since we have ordinal relationship between each category in target variable I have considered this one as numerical regression problem and try to observe the ANN behaviour Above one is underfit model it can be identified from the learning curve of the training loss only It is showing noisy values of relatively high loss indicating that the model was unable to learn the training dataset at all and model does not have a suitable capacity for the complexity of the dataset a id ann models multi class a Multiclass classification Target variable One hot encoded In this section we will create a classification model that uses categorical columns and tf idf features from accident description and one hot encoded target variable We can use simple densely connected neural networks to make predictions Above one is good fit it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values The loss of the model will almost always be lower on the training dataset than the validation dataset We could see it accuracy continually rise during training As expected we see the learning curves for accuracy on the test dataset plateau indicating that the model has no longer overfit the training dataset and it is generalized model Multiclass classification Target variable One hot encoded with SMOTE data In this section we will create a classification model that uses categorical columns and tf idf features from accident description and one hot encoded target variable We can use simple densely connected neural networks to make predictions Above one is overfit model it can be identified from the learning curve of the training and validation loss only Table of Contents table of contents a id nlp models a 12 Design train and test RNN or LSTM classifiers Architecture 1 Create a model with Text inputs only 2 Create a model with Categorical inputs only 3 Create a model with Multiple inputs a id nlp models text input a 1 Creating a Model with Text Inputs Only In this section we will create a classification model that uses accident description column alone Above one is good fit it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values The loss of the model will almost always be lower on the training dataset than the validation dataset We could see it accuracy continually rise during training As expected we see the learning curves for accuracy on the test dataset plateau indicating that the model has no longer overfit the training dataset and it is generalized model Note Surprisingly we observe that same f1 score 73 89 with accident description alone Table of Contents table of contents a id nlp models cat features a 2 Creating a Model with Categorical features Only In this section we will create a classification model that uses categorical columns alone Since the data for these columns is well structured and doesn t contain any sequential or spatial pattern we can use simple densely connected neural networks to make predictions Above one is good fit it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values The loss of the model will almost always be lower on the training dataset than the validation dataset We could see it accuracy continually rise during training As expected we see the learning curves for accuracy on the test dataset plateau indicating that the model has no longer overfit the training dataset and it is generalized model Table of Contents table of contents a id nlp models multiple input a 3 Creating a Model with Multiple Inputs The first submodel will accept textual input in the form of accident description This submodel will consist of an input shape layer an embedding layer and bidirectional LSTM layer of 128 neurons followed by max pool layer drop out and dense layers The second submodel will accept input in the form of meta information which consists of dense batch norm and drop out layers The output from the dropout layer of the first submodel and the output from the batch norm layer of the second submodel will be concatenated together and will be used as concatenated input to another dense layer with 10 neurons Finally the output dense layer will have five neuorns corresponding to each accident level Above one is good fit it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values The loss of the model will almost always be lower on the training dataset than the validation dataset We could see it accuracy continually rise during training As expected we see the learning curves for accuracy on the test dataset plateau indicating that the model has no longer overfit the training dataset and it is generalized model Table of Contents table of contents a id conclusion a 13 Conclusion 1 Able to predict the accident level with a test accuracy of 73 81 and f1 score of 73 81 2 We have seven duplicate values in this dataset and dropped those duplicate values 3 We have no outliers in this dataset 4 We have no missing values in this dataset 5 Extracted the day month and year from Date column and created new features such as weekday weekofyear and seasons 6 Target variable Accident Level distribution is not equal I 309 II 40 III 31 IV 30 V 8 7 Class imbalance issue is handled using below methods and found out that for this particular dataset with original data we have achieved the better results a Resampling techniques Oversampling minority class b SMOTE Generate synthetic samples 8 By comparing the results from all ML methods with original data we can select the best method as AdaBoost classifier with f1 score 65 38 with original data 9 Bootstrap sampling with RandomForestClassifier model with an accuracy of 66 7 77 6 is our best model 10 Bootstrap sampling with AdaBoostClassifier model with an accuracy of 51 7 76 8 is our best model 11 Bootstrap sampling with XGBClassifier model with an accuracy of 63 5 74 8 is our best model 12 Explored below options in Neural Networks a Convert Classification to Numerical problem achieved a test accuracy of 53 57 which is a bad result b Multiclass classification Target variable One hot encoded achieved a test accuracy of 73 81 and f1 score of 73 81 with original data TF IDF features from accident description column c Create a model with Text inputs accident description alone only surprisingly achieved a test accuracy of 73 81 and f1 score of 73 81 with original data d Create a model with Categorical features only achieved a test accuracy of 73 81 and f1 score of 72 28 with original data e Create a model with Multiple Inputs concatenated the layers from text input model and categorical features input model surprisingly achieved a test accuracy of 73 81 and f1 score of 73 81 with original data 13 Finally bidirectional LSTM model can be considered to productionalized the model and predict the accident level Table of Contents table of contents a id recommendations a 14 Recommendations In this project we discovered that the main causes of accidents are mistakes in hand operation and time related factor To reduce the occurrences of accidents more stringent safety standards in hand operation will be needed in period when many accidents occur I realized that the detail information of accidents like Description is so useful to analyze the cause With more detailed information such as machining data ex CNC Current Voltage in plants weather information employee s personal data ex age experience in the industry sector work performance we can clarify the cause of accidents more correctly With more number of observations than current number of records 425 so that we can feed more data into ML ANN NLP models to train evaluate the performance of those models and get the better results There are quite a lot of Critical risk descriptions but with the help of SME we can decide whether this column has outliers or not and also SME can help us in understanding the data better Table of Contents table of contents a id limitations a 15 Limitations We have less number of observations to analyse the cause of accidents correctly and rather we should collect more number of observations to get better results Less number of features available in dataset Lack of access to quality data Where does our model fall short in the real world Once we deploy the finalised model in Production we might get less f1 score as compared to productionalized model results Since we are predicting the accident level we need to be 100 sure or at least close to 100 so that we can prevent the lot of accidents in industry What can you do to enhance the solution Need to work on limitations a id closing reflections a 16 Closing Reflections What did we learned from the process How to work on Data Science project to end to end How to handle class imbalance data set How to build different ANN model architectures for handling multi class classification problems How to build different NLP architectures for handling both numerical and text data What I do differently next time Perhaps I will explore more feature engineering and feature selection techniques I will build the real chatbot using Streamlit or some other applications Table of Contents table of contents a id references a 17 References Good EDA Notebook https www kaggle com koheimuramatsu industrial accident causal analysis Holoviews plot tips http holoviews org user guide Customizing Plots html Scikit Learn API https scikit learn org stable Keras API https keras io api Streamlit API https docs streamlit io en stable api html https share streamlit io daniellewisdl streamlit cheat sheet app py Heroku with Python https devcenter heroku com articles getting started with python  used to supress display of warnings nlp libraries sampling methods import zscore for scaling the data save models pre processing methods the classification models ensemble models methods and classes for evaluation cross validation methods feature selection methods pre processing methods Deep learning libraries Keras pre processing Reproduce the results suppress display of warnings display all dataframe columns to set the limit to 3 decimals display all dataframe rows Read IHMStefanini industrial safety and health database with accidents description csv file Get the top 5 rows Check datatypes Check Data frame info Column names of Data frame Remove Unnamed 0 column from Data frame Rename Data Countries Genre Employee or Third Party columns in Data frame Get the top 2 rows Check duplicates in a data frame View the duplicate records Delete duplicate rows Get the shape of Industry data Check unique values of all columns except Description column Check the presence of missing values Visualize missing values function to create month variable into seasons Check the proportion of Industry sector in different countries Z test proportions More than 2 samples not implemented yet hence I am passing two elements Summary statistics Check the Correlation Checking 5 random Description and accident levels from the data Checking 5 random Descriptions and accident levels from the data where the length of headline is 100 Checking 5 random Descriptions and pot accident levels from the data where the length of headline is 100 Text preprocessing and stopwords custom module Get length of each line Get length of each line define training data train model summarize the loaded model summarize vocabulary save model load model this function creates a normalized vector for the whole sentence create sentence GLOVE embeddings vectors using the above function for training and validation set To replace white space everywhere in Employee type To replace white space everywhere in Critical Risk Create Industry DataFrame Label encoding convert integers to dummy variables i e one hot encoded Dummy variables encoding Merge the above dataframe with the original dataframe ind feat df Check NaN values Consider only top 30 GLOVE features Consider only top 30 GLOVE features Considering all Predictors Display old accident level counts Concatenate our training data back together Get the majority and minority class Upsample Level1 minority class sample with replacement to match majority class Upsample Level2 minority class sample with replacement to match majority class Upsample Level3 minority class sample with replacement to match majority class Upsample Level4 minority class sample with replacement to match majority class Combine majority class with upsampled minority classes Display new accident level counts Separate input features and target Considering all Predictors Separate input features and target Considering all Predictors Display new accident level counts convert integers to dummy variables i e one hot encoded Transform independent features Scaling only first 6 feautres Scaling only first 6 feautres generating the covariance matrix and the eigen values for the PCA analysis the relevanat covariance matrix generating the eigen values and the eigen vectors the cumulative variance explained analysis Plotting the variance expalained by the principal components and the cumulative variance explained Capturing 90 variance of the data DummyClassifier to predict all Accident levels checking unique labels checking accuracy Checking unique values Convert actual to a binary array if it s not already Fit the model on Training set Fit the model on Training set Intercept and Coefficients Predict on Test set Initialise mc logloss Model Confusion matrix Model Classification report Store the accuracy results for each model in a dataframe for final comparison Save the model return all the metrics along with predictions define classification models early stopping rounds 30 early stopping rounds 30 Train and Test the model Store the accuracy results for each model in a dataframe for final comparison note the start time Before starting with grid search we need to create a scoring function This is accomplished using the make scorer function of scikit learn define grid search summarize results note the end time calculate the total duration For multiclass problems only newton cg sag saga and lbfgs handle multinomial loss liblinear is limited to one versus rest schemes Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Random Forest Classifier on Training set Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Building a Linear Regression model Train and Test the model Store the accuracy results for each model in a dataframe for final comparison Train and Test all models with Lasso interaction terms Train and Test all models with Lasso interaction terms Train and Test all models with Lasso interaction terms define regressor models define model parameters penalty none l1 l2 elasticnet class weight none balanced multi class ovr multinomial class weight balanced balanced subsample none class weight balanced balanced subsample none Considering all Predictors Consider only top 30 GLOVE features Number of bootstrap samples to create size of a bootstrap sample run bootstrap empty list that will hold the scores for each bootstrap iteration prepare train and test sets Sampling with replacement picking rest of the data not considered in sample fit model fit against independent variables and corresponding target values Take the target column for all rows in test set evaluate model predict based on independent variables in the test data plot scores confidence intervals for 95 confidence tail regions on right and left 25 on each side indicated by P value border Number of bootstrap samples to create size of a bootstrap sample run bootstrap empty list that will hold the scores for each bootstrap iteration prepare train and test sets Sampling with replacement picking rest of the data not considered in sample fit model fit against independent variables and corresponding target values Take the target column for all rows in test set evaluate model predict based on independent variables in the test data plot scores confidence intervals for 95 confidence tail regions on right and left 25 on each side indicated by P value border Number of bootstrap samples to create size of a bootstrap sample run bootstrap empty list that will hold the scores for each bootstrap iteration prepare train and test sets Sampling with replacement picking rest of the data not considered in sample fit model fit against independent variables and corresponding target values Take the target column for all rows in test set evaluate model predict based on independent variables in the test data plot scores confidence intervals for 95 confidence tail regions on right and left 25 on each side indicated by P value border disable keras warnings get the accuracy precision recall f1 score from model predict probabilities for test set Multiclass predict crisp classes for test set Multiclass Multilabel reduce to 1d array accuracy tp tn p n precision tp tp fp recall tp tp fn f1 2 tp 2 tp fp fn Multiclass Multilabel fix random seed for reproducibility define the model compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves fix random seed for reproducibility define the model Multilabel compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format fix random seed for reproducibility define the model Multilabel compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format Select input and output features Encode labels in column Accident Level Divide our data into testing and training sets Convert both the training and test labels into one hot encoded vectors The first step in word embeddings is to convert the words into thier corresponding numeric indexes Sentences can have different lengths and therefore the sequences returned by the Tokenizer class also consist of variable lengths We need to pad the our sequences using the max length We need to load the built in GloVe word embeddings Build a LSTM Neural Network LSTM Layer 1 LSTM 128 embedding layer dense layer 1 Dense 5 activation softmax LSTM Layer 1 model Model inputs deep inputs outputs dense layer 1 Use earlystopping callback tf keras callbacks EarlyStopping monitor loss patience 5 min delta 0 001 fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format Select input and output features Encode labels in column Accident Level Divide our data into testing and training sets Convert both the training and test labels into one hot encoded vectors Variable transformation using StandardScaler Scaling only first 6 feautres Scaling only first 6 feautres fix random seed for reproducibility compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves serialize model to JSON serialize weights to HDF5 Save the model in h5 format fix random seed for reproducibility compile the keras model Use earlystopping fit the keras model on the dataset evaluate the keras model Get number of epochs plot loss learning curves plot accuracy learning curves ", "id": "vinayakshanawad/industrial-safety-complete-solution", "size": "36328", "language": "python", "html_url": "https://www.kaggle.com/code/vinayakshanawad/industrial-safety-complete-solution", "git_url": "https://www.kaggle.com/code/vinayakshanawad/industrial-safety-complete-solution"}